<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Hexo</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
<meta name="generator" content="Hexo 5.4.2"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Hexo</a></h1>
		<h2 class="header__subtitle"></h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2022/06/11/kafka-API使用方法/">kafka API使用方法</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-11</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" rel="tag">kafka API使用方法</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h4 id="1-首先引入maven文件"><a href="#1-首先引入maven文件" class="headerlink" title="1.首先引入maven文件"></a>1.首先引入maven文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;<span class="number">2.0</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kafka_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;<span class="number">2.2</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>⚠️：<strong>这里注意version和服务器的kafka版本要一致</strong></p>
<h4 id="2-生产者API"><a href="#2-生产者API" class="headerlink" title="2.生产者API"></a>2.生产者API</h4><h5 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h5><p><strong>创建生产者大概分为以下步骤</strong></p>
<blockquote>
<p>1.配置生产者客户端参数及创建相应的生产者实例<br>2.构建待发送的消息<br>3.发送消息<br>4.关闭生产者实例</p>
</blockquote>
<ul>
<li><p>代码示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">//设置 kafka 集群的地址</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span></span><br><span class="line">props.put(“acks”, “all”); </span><br><span class="line"></span><br><span class="line"><span class="comment">//失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">props.put(“retries”, <span class="number">3</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">props.put(“batch.size”, <span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">10000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送请求的最大缓存数</span></span><br><span class="line">props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端 buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">props.put(“buffer.memory”, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//key-value序列化器</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); </span><br></pre></td></tr></table></figure></li>
<li><p>消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord 类的定义如下:</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="2-2参数配置"><a href="#2-2参数配置" class="headerlink" title="2.2参数配置"></a>2.2参数配置</h5><ul>
<li>在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* bootstrap.servers </span><br><span class="line">* key.serializer </span><br><span class="line">* value.serializer</span><br></pre></td></tr></table></figure>

<ul>
<li>例如为了防止参数名字符串书写错误,可以进行以下设置:</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092&quot;</span>); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></figure>

<h5 id="2-3生产者api参数发送方式"><a href="#2-3生产者api参数发送方式" class="headerlink" title="2.3生产者api参数发送方式"></a>2.3生产者api参数发送方式</h5><p>这个客户端经过了生产环境测试并且通常情况它比原来Scals客户端更加快速、功能更加齐全。你可以通过添加以下示例的Maven坐标到客户端依赖中来使用这个新的客户端</p>
<ul>
<li><p>发后即忘( fire-and-forget )</p>
<p>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);</span><br></pre></td></tr></table></figure></li>
<li><p>同步发送( sync )</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	producer.send(rcd).get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">	e.printStackTrace();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>在调用 <code>send</code> 方法后可以接着调用 <code>get()</code> 方法，<code>send</code> 方法的返回值是一个 Future\对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">        <span class="comment">/*同步发送消息*/</span></span><br><span class="line">        <span class="type">RecordMetadata</span> <span class="variable">metadata</span> <span class="operator">=</span> producer.send(record).get();</span><br><span class="line">        System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 <code>Hello-Kafka</code> 主题时候，使用 <code>--partitions</code> 指定其分区数为 1，即只有一个分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49</span><br></pre></td></tr></table></figure></li>
<li><p>异步发送( async )</p>
<p>回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。</p>
<blockquote>
<p>⚠️：如若消息发送失败，会自动重新发送，无需手动使用回调函数</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">    <span class="comment">/*异步发送消息，并监听回调*/</span></span><br><span class="line">    producer.send(record, <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;进行异常处理&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="3-消费者API"><a href="#3-消费者API" class="headerlink" title="3.消费者API"></a>3.消费者API</h4><h5 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h5><p><strong>创建消费者大概分为以下步骤：</strong></p>
<blockquote>
<p>1.配置消费者客户端参数<br>2.创建相应的消费者实例;<br>3.订阅主题;<br>4.拉取消息并消费;<br>5.提交消费位移 offset;<br>6.关闭消费者实例。</p>
</blockquote>
<ul>
<li><p>代码示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 consumer group </span></span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;g1&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否自动提交 offset </span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 自动提交 offset 的时间间隔</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// key 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// value 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span></span><br><span class="line"><span class="comment">//Earliest目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span></span><br><span class="line"><span class="comment">//none（上次记录的偏移量，如果没有，会抛异常） </span></span><br><span class="line">props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;earliest&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 consumer KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); </span></span><br><span class="line"><span class="comment">// 消费者订阅的 topic, 可同时订阅多个</span></span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;test&quot;</span>,<span class="string">&quot;test1&quot;</span>)); </span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="3-2"><a href="#3-2" class="headerlink" title="3.2"></a>3.2</h5><ul>
<li>Kafka消费者可选属性</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. fetch.min.byte</span><br><span class="line">消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</span><br><span class="line"></span><br><span class="line">2. fetch.max.wait.ms</span><br><span class="line">broker 返回给消费者数据的等待时间，默认是 500ms。</span><br><span class="line"></span><br><span class="line">3. max.partition.fetch.bytes</span><br><span class="line">该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</span><br><span class="line"></span><br><span class="line">4. session.timeout.ms</span><br><span class="line">消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</span><br><span class="line"></span><br><span class="line">5. auto.offset.reset</span><br><span class="line">该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span><br><span class="line"></span><br><span class="line">latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;</span><br><span class="line">earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</span><br><span class="line">6. enable.auto.commit</span><br><span class="line">是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</span><br><span class="line"></span><br><span class="line">7. client.id</span><br><span class="line">客户端 id，服务器用来识别消息的来源。</span><br><span class="line"></span><br><span class="line">8. max.poll.records</span><br><span class="line">单次调用 poll() 方法能够返回的记录数量。</span><br><span class="line"></span><br><span class="line">9. receive.buffer.bytes &amp; send.buffer.byte</span><br><span class="line">这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</span><br></pre></td></tr></table></figure>

<ul>
<li>必要参数配置</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br></pre></td></tr></table></figure>

<h5 id="3-3-subscribe-订阅主题"><a href="#3-3-subscribe-订阅主题" class="headerlink" title="3.3  subscribe 订阅主题"></a>3.3  subscribe 订阅主题</h5><ul>
<li><p>subscribe 的重载方式:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br></pre></td></tr></table></figure></li>
<li><p>指定集合方式订阅主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer <span class="title function_">subscribe</span><span class="params">(Arrays.asList(topic2)</span>);</span><br></pre></td></tr></table></figure></li>
<li><p>正则方式订阅主题</p>
<p>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="3-4-assign-订阅主题"><a href="#3-4-assign-订阅主题" class="headerlink" title="3.4 assign 订阅主题"></a>3.4 assign 订阅主题</h5><p>消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区;</p>
<p>在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span> (<span class="string">&quot;tpc_1&quot;</span> , <span class="number">0</span>),<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(“tpc_2”,<span class="number">1</span>))) ;</span><br></pre></td></tr></table></figure>

<h5 id="3-5-subscribe-与-assign-的区别"><a href="#3-5-subscribe-与-assign-的区别" class="headerlink" title="3.5 subscribe 与 assign 的区别"></a>3.5 subscribe 与 assign 的区别</h5><ul>
<li><p>通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ;</p>
<blockquote>
<p>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</p>
</blockquote>
</li>
<li><p>assign() 方法订阅分区时,是不具备消费者自动均衡的功能的;</p>
<blockquote>
<p>其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。</p>
</blockquote>
</li>
</ul>
<h5 id="3-6-取消订阅"><a href="#3-6-取消订阅" class="headerlink" title="3.6 取消订阅"></a>3.6 取消订阅</h5><p>可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; 也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br></pre></td></tr></table></figure>

<p>如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;()) ; </span><br><span class="line">consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>

<h5 id="3-7-消息的消费模式"><a href="#3-7-消息的消费模式" class="headerlink" title="3.7 消息的消费模式"></a>3.7 消息的消费模式</h5><p>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</p>
<p>对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:<br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)<br>超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</p>
<p>​            <em><strong>消费者消费到的每条消息的类型为 ConsumerRecord</strong></em></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">NO_TIMESTAMP</span> <span class="operator">=</span> RecordBatch.NO_TIMESTAMP; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_SIZE</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_CHECKSUM</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</p>
<p>offsset 表示消息在所属分区的偏移量。</p>
<p>timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。</p>
<p>timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</p>
<p>headers 表示消息的头部内容。</p>
<p>key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </p>
<p>serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </p>
<p>checksum 是 CRC32 的校验值。</p>
</blockquote>
<h5 id="3-8指定位移消费"><a href="#3-8指定位移消费" class="headerlink" title="3.8指定位移消费"></a>3.8指定位移消费</h5><p>有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</p>
<p>​                <em><strong>seek()方法:</strong></em>        </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seek</span><span class="params">(TopicPartiton partition,<span class="type">long</span> offset)</span>;</span><br></pre></td></tr></table></figure>

<h5 id="3-9-再均衡监听器"><a href="#3-9-再均衡监听器" class="headerlink" title="3.9  再均衡监听器"></a>3.9  再均衡监听器</h5><p>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p>
<h5 id="3-10自动位移提交"><a href="#3-10自动位移提交" class="headerlink" title="3.10自动位移提交"></a>3.10自动位移提交</h5><p>Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.</p>
<p>auto.commit 参数为 true。</p>
<p>在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</p>
<p>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</p>
<ul>
<li><p>重复消费</p>
<p>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</p>
</li>
<li><p>丢失消息</p>
<ul>
<li>按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</li>
</ul>
</li>
</ul>
<h5 id="3-11手动位移提交-调用-kafka-api"><a href="#3-11手动位移提交-调用-kafka-api" class="headerlink" title="3.11手动位移提交(调用 kafka api)"></a>3.11手动位移提交(调用 kafka api)</h5><p>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。<br>很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals 。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br></pre></td></tr></table></figure>

<p>⚠️：<strong>手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。</strong></p>
<hr>
<h4 id="3-Topic管理-API"><a href="#3-Topic管理-API" class="headerlink" title="3.Topic管理 API"></a>3.Topic管理 API</h4><p>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)</p>
<h5 id="3-1列出主题"><a href="#3-1列出主题" class="headerlink" title="3.1列出主题"></a>3.1列出主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics(); </span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get(); </span><br><span class="line"></span><br><span class="line">System.out.println(topics);</span><br></pre></td></tr></table></figure>

<h5 id="3-2查看主题信息"><a href="#3-2查看主题信息" class="headerlink" title="3.2查看主题信息"></a>3.2查看主题信息</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; ksets = res.keySet(); </span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123; </span><br><span class="line">	System.out.println(res.get(k)); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-3-创建主题"><a href="#3-3-创建主题" class="headerlink" title="3.3 创建主题"></a>3.3 创建主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>); </span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,<span class="number">3000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 admin client 对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 由服务端 controller 自行分配分区及副本所在 broker </span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 手动指定分区及副本的 broker 分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); </span></span><br><span class="line"><span class="comment">// 分区 1,分配到 broker0,broker2 </span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>,Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments); </span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 future 中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">	result.all().get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">&#125; </span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure>

<h5 id="3-4删除主题"><a href="#3-4删除主题" class="headerlink" title="3.4删除主题"></a>3.4删除主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, <span class="string">&quot;tpc_1&quot;</span>)); </span><br><span class="line"></span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line"></span><br><span class="line">System.out.println(values);</span><br></pre></td></tr></table></figure>




	

	

</article>




	<article>
	
		<h1><a href="/2022/06/11/Kafka命令行操作/">Kafka命令行操作</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-11</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/" rel="tag">Kafka命令行操作</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h3 id="一、Kafka命令行操作"><a href="#一、Kafka命令行操作" class="headerlink" title="一、Kafka命令行操作"></a>一、Kafka命令行操作</h3><p> <strong>Kafka中的bin目录下有很多命令脚本，常用的例如：</strong></p>
<p>  kafka-topics.sh——-&gt;用于管理topic主题</p>
<p>  Kafka-config.sh——–&gt;用于对topic主题的配置管理</p>
<p>  kafka-console-producer.sh——–&gt;用于生产者进行写入数据</p>
<p>  kafka-console-consumer.sh——-&gt;用于消费者进行读取数据</p>
<h3 id="1-kafka-topics-sh"><a href="#1-kafka-topics-sh" class="headerlink" title="(1) kafka-topics.sh"></a>(1) kafka-topics.sh</h3><table>
<thead>
<tr>
<th align="center">–create</th>
<th align="center"><strong>指定创建topic主题动作</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>–topic</strong></td>
<td align="center"><strong>指定新建topic的名称</strong></td>
</tr>
<tr>
<td align="center"><strong>–zookeeper</strong></td>
<td align="center"><strong>指定kafka连接ZK的连接的url</strong></td>
</tr>
<tr>
<td align="center"><strong>–partitions</strong></td>
<td align="center"><strong>指定当前创建的kafka分区数量</strong></td>
</tr>
<tr>
<td align="center"><strong>–replication-factor</strong></td>
<td align="center"><strong>指定每个分区的复制因子个数</strong></td>
</tr>
</tbody></table>
<h4 id="1"><a href="#1" class="headerlink" title="1."></a>1.</h4><ul>
<li><strong>查看所有topic</strong></li>
</ul>
<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/%E5%9B%BE%E7%89%871.png" alt="markdown"><figcaption class="figure__caption">markdown</figcaption></figure></p>
<ul>
<li><p><strong>查看所有topic详细内容</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper --create node1:2181 --describe</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/%E5%9B%BE%E7%89%873.png" alt="markdown"><figcaption class="figure__caption">markdown</figcaption></figure></p>
</li>
<li><p><strong>增加分区</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --alter --topic tpc_2 --partitions 3 --zookeeper node1:2181</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/%E5%9B%BE%E7%89%875.png" alt="markdown"><figcaption class="figure__caption">markdown</figcaption></figure></p>
</li>
</ul>
<h4 id="2-创建新的topic"><a href="#2-创建新的topic" class="headerlink" title="2.创建新的topic"></a>2.创建新的topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper --create --replication-factor 3 --partitions 3 --topic tpc_11 node1:2181,node2:2181,node3:2181</span><br></pre></td></tr></table></figure>

<blockquote>
<p>指令中的含义：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--replication-factor 副本数量</span><br><span class="line">--partitions 分区数量</span><br><span class="line">--topic topic 名称</span><br></pre></td></tr></table></figure>
</blockquote>
<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/%E5%9B%BE%E7%89%872.png"></figure></p>
<h4 id="2-删除topic"><a href="#2-删除topic" class="headerlink" title="2.删除topic"></a>2.删除topic</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh  --delete --topic tpc_11 --zookeeper node1：2181</span><br></pre></td></tr></table></figure>

<hr>
<h3 id="（2）-Kafka-config-sh"><a href="#（2）-Kafka-config-sh" class="headerlink" title="（2） Kafka-config.sh"></a>（2） Kafka-config.sh</h3><ul>
<li>查看 topic 的配置</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh zookeeper node1: 2181 --describe --entity-type topics --entity-name tpc_2</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/6.png"></figure></p>
<ul>
<li></li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh zookeeper node1: 2181 --describe --entity-type brokers --entity-name 0 --zookeeper node1:2181</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/7.png"></figure></p>
<hr>
<h3 id="3-生产者与消费者操作"><a href="#3-生产者与消费者操作" class="headerlink" title="(3)生产者与消费者操作"></a>(3)生产者与消费者操作</h3><p>​            **生产者:kafka-console-producer **</p>
<p>​            <em><strong>消费者:kafka-console-consumer</strong></em></p>
<ul>
<li><p>数据写入与读取</p>
<ul>
<li><p>生产者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-producer.sh --broker-list node1:9092, node2:9092, node3:9092 --topic tpc_11</span><br></pre></td></tr></table></figure>



<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/%E5%9B%BE%E7%89%876.png"></figure></p>
</li>
<li><p>消费者</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-console-consumer.sh --bootstrap-server node1:9092, node2:9092, node3:9092 --topic tpc_11 --from-beginning</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/06/11/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/mojiyang/blog/source/_posts/Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C/%E5%9B%BE%E7%89%877.png"></figure></p>
</li>
</ul>
</li>
</ul>

	

	

</article>




	<article>
	
		<h1><a href="/2022/06/10/Kafka环境配置/">Kafka环境配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-10</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Kafka%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="tag">Kafka环境配置</a>
			</span>
		
	</div>

	

	
		<p><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></p>
<hr>
<p><em><strong>Kafka</strong></em>是一种高吞吐量的分布式发布订阅消息系统，其在大数据开发应用上的目的是通过 Hadoo的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。大数据开发需掌握Kafka架构原理及各组件的作用和使用方法及相关功能的实现。</p>
<h4 id="1-将kafka-2-11-2-0-0-tgz上传到node1中的-export-server文件夹中并解压"><a href="#1-将kafka-2-11-2-0-0-tgz上传到node1中的-export-server文件夹中并解压" class="headerlink" title="1.将kafka_2.11-2.0.0.tgz上传到node1中的/export/server文件夹中并解压"></a>1.将kafka_2.11-2.0.0.tgz上传到node1中的/export/server文件夹中并解压</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-2.0.0.tgz</span><br></pre></td></tr></table></figure>

<h4 id="2-创建软连接"><a href="#2-创建软连接" class="headerlink" title="2.创建软连接"></a>2.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s kafka_2.11-2.0.0/ kafka</span><br></pre></td></tr></table></figure>

<h4 id="3-进入-export-server-kafka-config"><a href="#3-进入-export-server-kafka-config" class="headerlink" title="3.进入/export/server/kafka/config"></a>3.进入/export/server/kafka/config</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka/config</span><br></pre></td></tr></table></figure>

<h4 id="4-修改配置文件-server-properties-分别修改第21行，31行，60行，65行，114行，123行，136行"><a href="#4-修改配置文件-server-properties-分别修改第21行，31行，60行，65行，114行，123行，136行" class="headerlink" title="4.修改配置文件 server.properties,分别修改第21行，31行，60行，65行，114行，123行，136行"></a>4.修改配置文件 server.properties,分别修改第21行，31行，60行，65行，114行，123行，136行</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">vim server.properties</span><br><span class="line"></span><br><span class="line"> 18 ############################# Server Basics #############################</span><br><span class="line"> 19 </span><br><span class="line"> 20 # The id of the broker. This must be set to a unique integer for each broker.</span><br><span class="line"> 21 broker.id=0																		#此处修改为本机的id，node2为1，node3为2。</span><br><span class="line"></span><br><span class="line"> 31 listeners=PLAINTEXT://192.168.138.151:9092</span><br><span class="line"> </span><br><span class="line"> 60 log.dirs=/export/server/data/kafka-logs				#设置默认日志存储路径</span><br><span class="line"> </span><br><span class="line">  65 num.partitions=1															#设置默认分区</span><br><span class="line">  </span><br><span class="line">  114 log.retention.check.interval.ms=300000			#300000ms=5min（超过即删除）</span><br><span class="line">  </span><br><span class="line">  123 zookeeper.connect=node1:2181,node2:2181,node3:2181</span><br><span class="line">  </span><br><span class="line">  136 group.initial.rebalance.delay.ms=0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="5-将node1中的kafka文件传输给node2-node3"><a href="#5-将node1中的kafka文件传输给node2-node3" class="headerlink" title="5.将node1中的kafka文件传输给node2 node3"></a>5.将node1中的kafka文件传输给node2 node3</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node2:$PWD</span><br><span class="line"></span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node3:$PWD</span><br></pre></td></tr></table></figure>

<h4 id="6-将node2-node3中的kafka-2-11-2-0-0创建软连接"><a href="#6-将node2-node3中的kafka-2-11-2-0-0创建软连接" class="headerlink" title="6.将node2 node3中的kafka_2.11-2.0.0创建软连接"></a>6.将node2 node3中的kafka_2.11-2.0.0创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/kafka_2.11-2.0.0/ kafka</span><br></pre></td></tr></table></figure>

<h4 id="7-配置全局环境变量"><a href="#7-配置全局环境变量" class="headerlink" title="7.配置全局环境变量"></a>7.配置全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kafka 环境变量</span></span><br><span class="line">export KAFKA_HOME=/export/server/kafka </span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin </span><br></pre></td></tr></table></figure>

<p>⚠️：需将每一台虚拟机的环境变量进行配置</p>
<h4 id="8-重载环境变量"><a href="#8-重载环境变量" class="headerlink" title="8.重载环境变量"></a>8.重载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="9-分别修改配置node2-node3中的-export-server-kafka-config文件夹中的-server-properties"><a href="#9-分别修改配置node2-node3中的-export-server-kafka-config文件夹中的-server-properties" class="headerlink" title="9.分别修改配置node2 node3中的/export/server/kafka/config文件夹中的 server.properties"></a>9.分别修改配置node2 node3中的/export/server/kafka/config文件夹中的 server.properties</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/kafka/config</span><br><span class="line">vim server.properties </span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>node2</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">21 broker.id=1</span><br><span class="line"></span><br><span class="line">31 listeners=PLAINTEXT://node2:9092 </span><br></pre></td></tr></table></figure></li>
<li><p><strong>node3</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">21 broker.id=2</span><br><span class="line"></span><br><span class="line">31 listeners=PLAINTEXT://node3:9092 </span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="10-Kafka服务启停"><a href="#10-Kafka服务启停" class="headerlink" title="10.Kafka服务启停"></a>10.Kafka服务启停</h4><ul>
<li><p>启动Kafka服务</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon /export/server/kafka/config/server.properties</span><br></pre></td></tr></table></figure>

<ul>
<li><p>```shell<br>(base) [root@node1 bin]# jps<br>14611 ResourceManager<br>5445 NodeManager<br>13942 NameNode<br>35815 Jps<br>4585 DataNode<br>35436 QuorumPeerMain<br>35772 Kafka</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* ```shell</span><br><span class="line">  (base) [root@node2 bin]# jps</span><br><span class="line">  9130 DataNode</span><br><span class="line">  64423 SecondaryNameNode</span><br><span class="line">  95242 Kafka</span><br><span class="line">  56341 Jps</span><br><span class="line">  36367 NodeManager</span><br><span class="line">  9854 QuorumPeerMain</span><br></pre></td></tr></table></figure></li>
<li><p>```shell<br>(base) [root@node3 bin]# jps<br>4242 Kafka<br>63673 NodeManager<br>2314 DataNode<br>24321 QuorumPeerMain<br>102104 Jps</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">    ⚠️：在启动kafka服务前需保证***zookeeper***服务的启动</span><br><span class="line"></span><br><span class="line">* 停止Kafka服务</span><br><span class="line"></span><br><span class="line">  ```shell</span><br><span class="line">  kafka-server-stop.sh stop</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>编写一键启动脚本（kfkall.sh）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cd /bin</span><br><span class="line">vim kfkall.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line">if [ $1 = start  ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-start.sh -daemon /export/server/kafka/config/server.properties&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = stop ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/kafka/bin/kafka-server-stop.sh&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p><strong>Kafka一键启动脚本</strong>的优点：方便快捷，节省分别在三台虚拟机分别启动的时间</p>
</li>
</ul>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/22/Spark基础环境配置/">Spark基础环境配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-22</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="tag">Spark基础环境配置</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><ul>
<li>Mac 用户注意需要注意，虚拟机使用VMware Fusion，镜像为CentOS7.</li>
</ul>
<hr>
<h2 id="一、基础配置"><a href="#一、基础配置" class="headerlink" title="一、基础配置"></a>一、基础配置</h2><h4 id="1-配置网络适配"><a href="#1-配置网络适配" class="headerlink" title="1.配置网络适配"></a><strong>1.配置网络适配</strong></h4><p>Mac用户只需点击VMware Fusion &gt; 偏好设置 &gt; 网络，并点击左下角“ + ”即可添加虚拟机上所用的子网ip（例：192.168.138.0）。</p>
<p><figure class="figure"><img src="/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/1.png" alt="1.png"><figcaption class="figure__caption">1.png</figcaption></figure></p>
<p><em>注：Mac用户若省略此处步骤可能会导致时间的推移而导致ping不动网络（血的教训😭）</em></p>
<ul>
<li><p>分别修改三台虚拟机的主机名</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node1</span></span><br><span class="line">echo &quot;node1&quot; &gt;/etc/hostname</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node2</span></span><br><span class="line">echo &quot;node2&quot; &gt;/etc/hostname</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node3</span></span><br><span class="line">echo &quot;node3&quot; &gt;/etc/hostname</span><br></pre></td></tr></table></figure></li>
<li><p>更该网络配置 （node1 node2 node3）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33(node2,node3中的IPADDR只需在151上递加即可)</span><br><span class="line"></span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;# 配置BOOTPROTO=static：表示静态路由协议，可以保持IP固定</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;7815751b-505d-4ae2-b2d4-aa39591dc6aa&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;# 配置ONBOOT=yes:表示启动这块网卡</span><br><span class="line">IPADDR=&quot;192.168.138.151&quot;# 配置IPADDR:表示虚拟机的IP地址，这里设置的IP地址要与前面IP映射配置时的IP地址一致，否则无法通过主机名找到对应IP;</span><br><span class="line">PREFIX=&quot;24&quot;</span><br><span class="line">GATEWAY=&quot;192.168.138.2&quot;# 配置GATEWAY:表示虚拟机网关,通常都是将IP地址最后一个位数变为2;</span><br><span class="line">DNS1=&quot;192.168.138.2&quot;# 配置DNS1:表示域名解析器，此处采用Google提供的免费DNS服务器88.8.8(也可以设置为PC端电脑对应的DNS)。</span><br><span class="line">DOMAIN=&quot;114.114.114.114&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>更改完成后对三台虚拟机进行重启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutdown -r 0</span><br></pre></td></tr></table></figure></li>
<li><p>重启成功后进行一次连通外网的测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping baidu.com -c3</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-修改host映射"><a href="#2-修改host映射" class="headerlink" title="2.修改host映射"></a>2.修改host映射</h4><p>修改/etc/hosts映射文件，为的是将来对相关联的虚拟机操作更加便捷，其中添加相互对应的IP地址和主机名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.138.151 node1 node1.itcast.cn</span><br><span class="line">192.168.138.152 node2 node2.itcast.cn</span><br><span class="line">192.168.138.153 node3 node3.itcast.cn                                 </span><br></pre></td></tr></table></figure>

<p>新手须知：推出并保存的方式可为 ‘esc + x‘，’esc + shift+zz‘</p>
<h4 id="3-集群的时间同步"><a href="#3-集群的时间同步" class="headerlink" title="3.集群的时间同步"></a>3.集群的时间同步</h4><p>我们有的时候在使用虚拟机时遇到数据误差的时候，总是不清楚哪里出错，原因就在可能没有进行时间同步。在整个集群当中，时间同步充当十分重要的角色。不要存在侥幸心理，如若因为时间不同步造成了损失，即使是进行了数据备份，那么你可能无法在正确的时间将正确的数据进行备份。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp5.aliyun.com</span><br></pre></td></tr></table></figure>

<p>建议：在每次执行poweroff之后的再启动后执行。</p>
<h4 id="4-SSH免密钥登陆"><a href="#4-SSH免密钥登陆" class="headerlink" title="4. SSH免密钥登陆"></a>4. SSH免密钥登陆</h4><ul>
<li><p>在node1上生成公钥私钥</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br><span class="line"></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:QUAgFH5KBc/Erlf1JWSBbKeEepPJqMBqpWbc02/uFj8 root@master</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">| .=++oo+.o+.     |</span><br><span class="line">| . *. ..*.o .    |</span><br><span class="line">|. o.++ *.+ o     |</span><br><span class="line">|.o ++ B ...      |</span><br><span class="line">|o.=o.o .S        |</span><br><span class="line">|.*oo.. .         |</span><br><span class="line">|+  .. . o        |</span><br><span class="line">|       + E       |</span><br><span class="line">|      =o  .      |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure></li>
<li><p>在node1上配置免密登陆node1 node2 node3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id master</span><br><span class="line">ssh-copy-id slave1</span><br><span class="line">ssh-copy-id slave2</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="二、安装配置JDK"><a href="#二、安装配置JDK" class="headerlink" title="二、安装配置JDK"></a>二、安装配置JDK</h2><h4 id="1-创建编译环境软件所需的根目录"><a href="#1-创建编译环境软件所需的根目录" class="headerlink" title="1.创建编译环境软件所需的根目录"></a>1.创建编译环境软件所需的根目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/server</span><br></pre></td></tr></table></figure>

<h4 id="2-安装JDK1-8-（首先将-jdk-8u241-linux-x64-tar-gz压缩包上传到-export-server目录，并解压）"><a href="#2-安装JDK1-8-（首先将-jdk-8u241-linux-x64-tar-gz压缩包上传到-export-server目录，并解压）" class="headerlink" title="2.安装JDK1.8 （首先将 jdk-8u241-linux-x64.tar.gz压缩包上传到/export/server目录，并解压）"></a>2.安装JDK1.8 （首先将 jdk-8u241-linux-x64.tar.gz压缩包上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u241-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="3-配置全局环境变量"><a href="#3-配置全局环境变量" class="headerlink" title="3.配置全局环境变量"></a>3.配置全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure>

<h4 id="4-加载环境变量"><a href="#4-加载环境变量" class="headerlink" title="4.加载环境变量"></a>4.加载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="5-查看java版本"><a href="#5-查看java版本" class="headerlink" title="5.查看java版本"></a>5.查看java版本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">(base) [root@node1 ~]# java -version</span><br><span class="line">java version &quot;1.8.0_241&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_241-b07)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</span><br></pre></td></tr></table></figure>

<p>出现以下内容则代表安装成功</p>
<h4 id="6-将java文件从node1中传输到node2-node3中，传输完成后，配置方法参考-3-4"><a href="#6-将java文件从node1中传输到node2-node3中，传输完成后，配置方法参考-3-4" class="headerlink" title="6.将java文件从node1中传输到node2 node3中，传输完成后，配置方法参考 3. 4."></a>6.将java文件从node1中传输到node2 node3中，传输完成后，配置方法参考 3. 4.</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>

<h4 id="7-在三台主机分别创建软连接（为了方便）"><a href="#7-在三台主机分别创建软连接（为了方便）" class="headerlink" title="7. 在三台主机分别创建软连接（为了方便）"></a>7. 在三台主机分别创建软连接（为了方便）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">ln -s jdk1.8.0_241/jdk</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="三、安装配置Hadoop"><a href="#三、安装配置Hadoop" class="headerlink" title="三、安装配置Hadoop"></a>三、安装配置Hadoop</h2><h4 id="1-安装Hadoop-（首先将hadoop-3-3-0-Centos7-with-wnappy-tar-gz压缩包上传到-export-server目录，并解压）"><a href="#1-安装Hadoop-（首先将hadoop-3-3-0-Centos7-with-wnappy-tar-gz压缩包上传到-export-server目录，并解压）" class="headerlink" title="1.安装Hadoop （首先将hadoop-3.3.0-Centos7-with-wnappy.tar.gz压缩包上传到/export/server目录，并解压）"></a>1.安装Hadoop （首先将hadoop-3.3.0-Centos7-with-wnappy.tar.gz压缩包上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.0-Centos7-with-wnappy.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="2-创建软连接"><a href="#2-创建软连接" class="headerlink" title="2.创建软连接"></a>2.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s jhadoop-3.3.0/ hadoop</span><br></pre></td></tr></table></figure>

<h4 id="3-修改配置文件"><a href="#3-修改配置文件" class="headerlink" title="3.修改配置文件"></a>3.修改配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>

<ul>
<li><p>修改Hadoop-env.sh文件 </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">末尾添加</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line"></span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure></li>
<li><p>修改core-site.xml文件</p>
<ul>
<li><p>设置默认系统</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://node1:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置hadoop本地保存数据路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置HDFS web UI用户身份</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>整合hive用户代理设置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置文件系统垃圾桶保存时间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>配置mapred-site.xml</p>
<ul>
<li><p>设置MR程序默认运行模式：yarn集群模式 local本地模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置MR程序历史服务地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置MR程序历史服务器web端地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>修改yarn-site.xml</p>
<ul>
<li><p>设置YARN集群主角色运行机器位置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;node1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>是否将对容器实施物理内存限制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置yarn历史服务器地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>历史日志保存的时间 7天</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="4-将Hadoop文件从node1中传输到node2-node3中"><a href="#4-将Hadoop文件从node1中传输到node2-node3中" class="headerlink" title="4. 将Hadoop文件从node1中传输到node2 node3中"></a>4. 将Hadoop文件从node1中传输到node2 node3中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/hadoop/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/hadoop/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>

<h4 id="5-三台主机分别配置全局变量"><a href="#5-三台主机分别配置全局变量" class="headerlink" title="5. 三台主机分别配置全局变量"></a>5. 三台主机分别配置全局变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<h4 id="6-重新加载全局环境变量"><a href="#6-重新加载全局环境变量" class="headerlink" title="6.重新加载全局环境变量"></a>6.重新加载全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="7-Hadoop集群启动前需要进行格式化"><a href="#7-Hadoop集群启动前需要进行格式化" class="headerlink" title="7.Hadoop集群启动前需要进行格式化"></a>7.Hadoop集群启动前需要进行格式化</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h4 id="8-启动hadoop集群"><a href="#8-启动hadoop集群" class="headerlink" title="8.启动hadoop集群"></a>8.启动hadoop集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hadoop/bin</span><br><span class="line"></span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="9-查看当前所有进程服务"><a href="#9-查看当前所有进程服务" class="headerlink" title="9.查看当前所有进程服务"></a>9.查看当前所有进程服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">3040 ResourceManager</span><br><span class="line">3713 Jps</span><br><span class="line">3240 NodeManager</span><br><span class="line">2603 DataNode</span><br><span class="line">2335 NameNode</span><br><span class="line"></span><br><span class="line">(base) [root@node2 ~]# jps</span><br><span class="line">2322 Jps</span><br><span class="line">2101 SecondaryNameNode</span><br><span class="line">2199 NodeManager</span><br><span class="line">1978 DataNode</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line"></span><br><span class="line">(base) [root@node3 ~]# jps</span><br><span class="line">2225 Jps</span><br><span class="line">1972 DataNode</span><br><span class="line">2101 NodeManager</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure>

<h4 id="10-进入hdfs-yarn-web-UI页面"><a href="#10-进入hdfs-yarn-web-UI页面" class="headerlink" title="10.进入hdfs yarn web UI页面"></a>10.进入hdfs yarn web UI页面</h4><p><figure class="figure"><img src="/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/2.png" alt="2.png"><figcaption class="figure__caption">2.png</figcaption></figure></p>
<p><figure class="figure"><img src="/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/3.png" alt="3.png"><figcaption class="figure__caption">3.png</figcaption></figure></p>
<hr>
<h2 id="四、安装配置zookeeper"><a href="#四、安装配置zookeeper" class="headerlink" title="四、安装配置zookeeper"></a>四、安装配置zookeeper</h2><h4 id="1-安装zookeeper-（首先将zookeeper-3-4-10-tar-gz压缩包上传到-export-server目录，并解压）"><a href="#1-安装zookeeper-（首先将zookeeper-3-4-10-tar-gz压缩包上传到-export-server目录，并解压）" class="headerlink" title="1.安装zookeeper （首先将zookeeper-3.4.10.tar.gz压缩包上传到/export/server目录，并解压）"></a>1.安装zookeeper （首先将zookeeper-3.4.10.tar.gz压缩包上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="2-创建软连接-1"><a href="#2-创建软连接-1" class="headerlink" title="2.创建软连接"></a>2.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure>

<h4 id="3-修改配置文件-1"><a href="#3-修改配置文件-1" class="headerlink" title="3.修改配置文件"></a>3.修改配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/conf/ </span><br></pre></td></tr></table></figure>

<ul>
<li><p>将 zoo_sample.cfg 文件复制为新文件 zoo.cfg </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure></li>
<li><p>配置zoo.cfg文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim zoo.cfg</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">Zookeeper的数据存放目录</span></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">集群中服务器地址</span></span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br><span class="line">server.3=node3:2888:3888</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="4-进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-1-写入进去"><a href="#4-进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-1-写入进去" class="headerlink" title="4.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去"></a>4.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/zkdata</span><br><span class="line"></span><br><span class="line">touch myid</span><br><span class="line"></span><br><span class="line">echo &#x27;1&#x27; &gt; myid</span><br></pre></td></tr></table></figure>

<h4 id="5-将zookeeper文件从node1中传输到node2-node3中"><a href="#5-将zookeeper文件从node1中传输到node2-node3中" class="headerlink" title="5.将zookeeper文件从node1中传输到node2 node3中"></a>5.将zookeeper文件从node1中传输到node2 node3中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/zookeeper-3.4.10/ node2:$PWD</span><br><span class="line">scp -r /export/server/zookeeper-3.4.10/ node3:$PWD</span><br></pre></td></tr></table></figure>

<h4 id="6-创建软连接"><a href="#6-创建软连接" class="headerlink" title="6.创建软连接"></a>6.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure>

<h4 id="7-node2-node3分别进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-2-，3-写入进去"><a href="#7-node2-node3分别进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-2-，3-写入进去" class="headerlink" title="7.node2 node3分别进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 2 ，3 写入进去"></a>7.node2 node3分别进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 2 ，3 写入进去</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/zkdatas/</span><br><span class="line"></span><br><span class="line">[root@node2 zkdatas]# vim myid </span><br><span class="line">[root@node2 zkdatas]# more myid </span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">[root@node3 zkdatas]# vim myid </span><br><span class="line">[root@node3 zkdatas]# more myid </span><br><span class="line">3</span><br></pre></td></tr></table></figure>

<h4 id="8-配置全局环境变量"><a href="#8-配置全局环境变量" class="headerlink" title="8.配置全局环境变量"></a>8.配置全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">zookeeper 环境变量</span></span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<h4 id="9-重新加载环境变量"><a href="#9-重新加载环境变量" class="headerlink" title="9.重新加载环境变量"></a>9.重新加载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="10-启动zookeeper服务"><a href="#10-启动zookeeper服务" class="headerlink" title="10.启动zookeeper服务"></a>10.启动zookeeper服务</h4><ul>
<li><p>普通启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/server/zookeeper-3.4.10/bin </span><br><span class="line"></span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure></li>
<li><p>编写脚本进行一键启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">cd /bin</span><br><span class="line"></span><br><span class="line">vim zkall.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line">if [ $1 = start  ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = stop ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = status ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;node$&#123;i&#125; status:&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<ul>
<li><p>给编写的脚本赋予可执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x zkall.sh</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>注：启动时，无需进入任意文件目录下，直接执行zkall.sh start 即可</strong></p>
</li>
</ul>
<h4 id="11-查看进程"><a href="#11-查看进程" class="headerlink" title="11.查看进程"></a>11.查看进程</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">4957 Jps</span><br><span class="line">4911 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">(base) [root@node2 ~]# jps</span><br><span class="line">2736 QuorumPeerMain</span><br><span class="line">2795 Jps</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line"></span><br><span class="line">(base) [root@node3 ~]# jps</span><br><span class="line">2576 Jps</span><br><span class="line">2533 QuorumPeerMain</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure>


	

	

</article>




	<article>
	
		<h1><a href="/2022/05/22/Spark-local-stand-alone配置/">Spark local&amp; stand-alone配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-22</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/" rel="tag">Spark local& stand-alone配置</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h2 id="安装配置Spark"><a href="#安装配置Spark" class="headerlink" title="安装配置Spark"></a>安装配置Spark</h2><p><em><strong>Spark是专为大规模数据处理而设计的快速通用的计算引擎，其提供了一个全面、统一的框架用于管理各种不同性质的数据集和数据源的大数据处理的需求，大数据开发需掌握Spark基础、SparkJob、Spark RDD、spark job部署与资源分配、Spark shuffle、Spark内存管理、Spark广播变量、Spark SQL、Spark Streaming以及Spark ML等相关知识。</strong></em></p>
<hr>
<h2 id="一、Spark-local模式"><a href="#一、Spark-local模式" class="headerlink" title="一、Spark-local模式"></a>一、Spark-local模式</h2><p><em><strong>local模式是以一个单独的进程，通过其内部的多个线程来模拟整个spark</strong></em></p>
<h4 id="1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式只需在node1上安装即可）"><a href="#1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式只需在node1上安装即可）" class="headerlink" title="1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式只需在node1上安装即可）"></a>1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式只需在node1上安装即可）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">执行过程中需注意：</span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现内容选 <span class="built_in">yes</span></span></span><br><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; <span class="built_in">yes</span></span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现添加路径：/export/server/anaconda3</span></span><br><span class="line">...</span><br><span class="line">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3</span><br><span class="line">PREFIX=/export/server/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>安装成功后，现推出，再重新登录终端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br><span class="line"></span><br><span class="line">(base) [root@node1 ~]# </span><br></pre></td></tr></table></figure>

<p>⚠️：出现（base）则代表安装成功！</p>
<h4 id="2-创建基于python3-8虚拟环境的pyspark"><a href="#2-创建基于python3-8虚拟环境的pyspark" class="headerlink" title="2.创建基于python3.8虚拟环境的pyspark"></a>2.创建基于python3.8虚拟环境的pyspark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 </span><br></pre></td></tr></table></figure>

<h4 id="3-切换到pyspark虚拟环境内"><a href="#3-切换到pyspark虚拟环境内" class="headerlink" title="3.切换到pyspark虚拟环境内"></a>3.切换到pyspark虚拟环境内</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# conda activate pyspark  </span><br><span class="line">(pyspark) [root@master ~]# </span><br></pre></td></tr></table></figure>

<p>⚠️：出现（pyspark）则代表成功！</p>
<h4 id="4-在虚拟环境内安装所需包"><a href="#4-在虚拟环境内安装所需包" class="headerlink" title="4.在虚拟环境内安装所需包"></a>4.在虚拟环境内安装所需包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<p>⚠️：中间过程可能会出现WARNING 不管即可</p>
<h4 id="5-安装配置Spark（首先将spark-3-2-0-bin-hadoop3-2-tgz上传到-export-server目录，并解压）"><a href="#5-安装配置Spark（首先将spark-3-2-0-bin-hadoop3-2-tgz上传到-export-server目录，并解压）" class="headerlink" title="5. 安装配置Spark（首先将spark-3.2.0-bin-hadoop3.2.tgz上传到/export/server目录，并解压）"></a>5. 安装配置Spark（首先将spark-3.2.0-bin-hadoop3.2.tgz上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure>

<h4 id="6-创建软连接"><a href="#6-创建软连接" class="headerlink" title="6.创建软连接"></a>6.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<h4 id="7-配置全局环境变量-并重新加载"><a href="#7-配置全局环境变量-并重新加载" class="headerlink" title="7.配置全局环境变量,并重新加载"></a>7.配置全局环境变量,并重新加载</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME							JAVA_HOME: 告知Spark Java的所在路径位置</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME						HADOOP_HOME: 告知Spark  Hadoop的所在路径位置</span></span><br><span class="line">export HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">ZOOKEEPER_HOME</span></span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SPARK_HOME      				SPARK_HOME: 表示Spark安装路径</span></span><br><span class="line">export SPARK_HOME=/export/server/spark</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_CONF_DIR				告知Spark Hadoop的配置文件的路径位置</span></span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">PYSPARK_PYTHON      		PYSPARK_PYTHON: 表示Spark将要运行Python程序, python解释器的位置</span></span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新加载</span></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241  </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">PYSPARK_PYTHON</span></span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新加载</span></span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<h4 id="8-开启"><a href="#8-开启" class="headerlink" title="8.开启"></a>8.开启</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line">./pyspark</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/05/22/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/4.png" alt="4.png"><figcaption class="figure__caption">4.png</figcaption></figure></p>
<h4 id="9-查看web-UI界面"><a href="#9-查看web-UI界面" class="headerlink" title="9.查看web UI界面"></a>9.查看web UI界面</h4><h4 id="10-退出"><a href="#10-退出" class="headerlink" title="10.退出"></a>10.退出</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="二、Spark-stand-alone模式"><a href="#二、Spark-stand-alone模式" class="headerlink" title="二、Spark-stand-alone模式"></a>二、Spark-stand-alone模式</h2><p><em><strong>Stand-alone模式中各个角色以独立个体进程的方式存在，一起组成Spark集群</strong></em></p>
<h4 id="1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2-node3全部单独部署）"><a href="#1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2-node3全部单独部署）" class="headerlink" title="1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2 node3全部单独部署）"></a>1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2 node3全部单独部署）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">执行过程中需注意：</span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现内容选 <span class="built_in">yes</span></span></span><br><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; <span class="built_in">yes</span></span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现添加路径：/export/server/anaconda3</span></span><br><span class="line">...</span><br><span class="line">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3</span><br><span class="line">PREFIX=/export/server/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>安装成功后，现推出，再重新登录终端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br><span class="line"></span><br><span class="line">(base) [root@node1 ~]# </span><br></pre></td></tr></table></figure>

<p>⚠️：node2 node3 分别全部执行！！！</p>
<h4 id="2-把所需的全局环境变量全部传输到node2-node3"><a href="#2-把所需的全局环境变量全部传输到node2-node3" class="headerlink" title="2.把所需的全局环境变量全部传输到node2 node3"></a>2.把所需的全局环境变量全部传输到node2 node3</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">传输 .bashrc :</span></span><br><span class="line">scp ~/.bashrc root@node2:~/</span><br><span class="line">scp ~/.bashrc root@node3:~/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">传输 profile :</span></span><br><span class="line">scp /etc/profile/ root@node2:/etc/</span><br><span class="line">scp /etc/profile/ root@node3:/etc/</span><br></pre></td></tr></table></figure>

<h4 id="3-创建基于python3-8虚拟环境的pyspark"><a href="#3-创建基于python3-8虚拟环境的pyspark" class="headerlink" title="3.创建基于python3.8虚拟环境的pyspark"></a>3.创建基于python3.8虚拟环境的pyspark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 </span><br></pre></td></tr></table></figure>

<h4 id="4-切换到pyspark虚拟环境内"><a href="#4-切换到pyspark虚拟环境内" class="headerlink" title="4.切换到pyspark虚拟环境内"></a>4.切换到pyspark虚拟环境内</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark </span><br><span class="line"></span><br><span class="line">(base) [root@node2 ~]# conda activate pyspark  </span><br><span class="line">(pyspark) [root@node2 ~]# </span><br></pre></td></tr></table></figure>

<h4 id="5-在虚拟环境内安装所需包"><a href="#5-在虚拟环境内安装所需包" class="headerlink" title="5.在虚拟环境内安装所需包"></a>5.在虚拟环境内安装所需包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<h4 id="6-配置相关文件"><a href="#6-配置相关文件" class="headerlink" title="6.配置相关文件"></a>6.配置相关文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/conf</span><br></pre></td></tr></table></figure>

<p>⚠️：此操作在node1上执行</p>
<ul>
<li><p>将文件 workers.template 改名为 workers，并配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br><span class="line"></span><br><span class="line">vim workers</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">localhost删除，内容追加文末：</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure></li>
<li><p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"></span><br><span class="line">文末追加内容：</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置JAVA安装目录</span></span></span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 指定spark老大Master的IP和提交任务的通信端口</span></span></span><br><span class="line">export SPARK_MASTER_HOST=node1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知sparkmaster的通讯端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置历史服务器 ------将spark上运行的日志记录储存到hdfs中的sparklog中</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>此刻打开一个新的shell窗口，然后开启Hadoop</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure>

<p>⚠️：不建议 在启动单独集群时 需要执行单独对应的.sh文件</p>
</li>
<li><p>在HDFS上创建可存放历史日志的文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>将 spark-defaults.conf.template改为 spark-defaults.conf ，并配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"></span><br><span class="line">vim spark-defaults.conf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启spark的日期记录功能</span></span><br><span class="line">spark.eventLog.enabled 	true</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志记录的路径</span></span><br><span class="line">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志是否启动压缩</span></span><br><span class="line">spark.eventLog.compress 	true</span><br></pre></td></tr></table></figure></li>
<li><p>将og4j.properties.template改为log4j.properties，并配置文件 (修改第十九行：将INFO改为WARN)</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line"></span><br><span class="line">vim log4j.properties</span><br><span class="line"></span><br><span class="line"> 18 # Set everything to be logged to the console</span><br><span class="line"> 19 log4j.rootCategory=WARN, console</span><br><span class="line"> 20 log4j.appender.console=org.apache.log4j.ConsoleAppender</span><br><span class="line"> 21 log4j.appender.console.target=System.err</span><br><span class="line"> 22 log4j.appender.console.layout=org.apache.log4j.PatternLayout</span><br><span class="line"> 23 log4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n</span><br></pre></td></tr></table></figure>

<p>**此步骤的目的是让输出的日志之输出警告和错误的日志 **   <em>INFO表示输出所有日志</em></p>
<h4 id="7-将Spark文件夹全部传输到node2-node3上"><a href="#7-将Spark文件夹全部传输到node2-node3上" class="headerlink" title="7.将Spark文件夹全部传输到node2 node3上"></a>7.将Spark文件夹全部传输到node2 node3上</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWD</span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD</span><br></pre></td></tr></table></figure>

<h4 id="8-创建软连接"><a href="#8-创建软连接" class="headerlink" title="8.创建软连接"></a>8.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<h4 id="9-重新加载环境变量"><a href="#9-重新加载环境变量" class="headerlink" title="9.重新加载环境变量"></a>9.重新加载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p>⚠️：因为在步骤2中已经将node1中的全局环境变量传输到node2 node3 中，所以执行即可</p>
<h4 id="10-启动history-server服务"><a href="#10-启动history-server服务" class="headerlink" title="10.启动history-server服务"></a>10.启动history-server服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/sbin </span><br><span class="line"></span><br><span class="line">./start-history-server.sh</span><br><span class="line"></span><br><span class="line">(base)[root@node1 sbin]# jps</span><br><span class="line">7393 HistoryServer</span><br><span class="line">6034 NameNode</span><br><span class="line">7012 NodeManager</span><br><span class="line">2603 DataNode</span><br><span class="line">6699 ResourceManager</span><br><span class="line">4911 QuorumPeerMain</span><br><span class="line">7439 Jps</span><br></pre></td></tr></table></figure>

<h4 id="11-查看-webUI"><a href="#11-查看-webUI" class="headerlink" title="11.查看 webUI"></a>11.查看 webUI</h4><p><figure class="figure"><img src="/2022/05/22/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/5.png" alt="5.png"><figcaption class="figure__caption">5.png</figcaption></figure></p>
<h4 id="12-启动-关闭spark-的主从节点-master-worker进程指令"><a href="#12-启动-关闭spark-的主从节点-master-worker进程指令" class="headerlink" title="12.启动/关闭spark 的主从节点 master worker进程指令"></a>12.启动/关闭spark 的主从节点 master worker进程指令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh 			#启动全部的master和worke    在node1上执行则默认node1为master</span><br><span class="line">sbin/start-master.sh		#只启动执行主机上master</span><br><span class="line">sbin/start-worker.sh		#只启动执行主机上worker</span><br><span class="line">sbin/stop-all.sh				#停止所有</span><br><span class="line">sbin/stop-master.sh			#只停止执行主机上的master</span><br><span class="line">sbin/stop-worker.sh			#只停止执行主机上的worker</span><br></pre></td></tr></table></figure>

<h4 id="13-查看webUI"><a href="#13-查看webUI" class="headerlink" title="13. 查看webUI"></a>13. 查看webUI</h4><p><figure class="figure"><img src="/2022/05/22/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/6.png" alt="6.png"><figcaption class="figure__caption">6.png</figcaption></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/22/Spark-HA-Yarn配置/">Spark HA &amp; Yarn配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-22</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/" rel="tag">Spark HA & Yarn配置</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h3 id="一、Spark-Standalone-HA模式"><a href="#一、Spark-Standalone-HA模式" class="headerlink" title="一、Spark-Standalone-HA模式"></a>一、Spark-Standalone-HA模式</h3><p><em><strong>Standalone集群是Master-Slaves架构的集群模式，和大部分的Master-Slaves结构集群一样，存在着Master单点故障的问题。 如何解决这个单点故障的问题，Spark提供了两种方案：1.基于文件系统的单点恢复(Single-Node Recovery with Local File System)–只能用于开发或测试环境。2.基于zookeeper的Standby Masters(Standby Masters with ZooKeeper)–可以用于生产环境。</strong></em></p>
<p>⚠️：主机中的zookeeper的版本需要喝spark版本兼容，否则会出现错误。若不兼容则重新下载与之兼容的zookeeper，并删除之前的软连接，再重新配置，最后分配给node2 node3即可。</p>
<h4 id="1-配置修改-spark-env-sh-文件内容"><a href="#1-配置修改-spark-env-sh-文件内容" class="headerlink" title="1.配置修改 spark-env.sh 文件内容"></a>1.配置修改 spark-env.sh 文件内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/conf </span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#为 83 行内容加上注释</span></span></span><br><span class="line">82 # 告知Spark的master运行在哪个机器上</span><br><span class="line">83 # export SPARK_MASTER_HOST=master</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#文末填写以下内容</span></span></span><br><span class="line">105 SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy    .zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir    =/spark-ha&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定Zookeeper的连接地址</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></figure>

<h4 id="2-将spark-env-sh传输到node2-node3上"><a href="#2-将spark-env-sh传输到node2-node3上" class="headerlink" title="2.将spark-env.sh传输到node2 node3上"></a>2.将spark-env.sh传输到node2 node3上</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line"></span><br><span class="line">scp spark-env.sh node3:/export/server/spark/conf/</span><br></pre></td></tr></table></figure>

<h4 id="3-启动集群"><a href="#3-启动集群" class="headerlink" title="3.启动集群"></a>3.启动集群</h4><p>⚠️：在启动集群前需确保相关zookeeper和Hadoop均已启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#首先在node1上启动master 和所有主机上的worker</span></span></span><br><span class="line">cd /export/server/spark/sbin</span><br><span class="line">./start-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#再到node2上启动 node2上的master作为备用master</span></span></span><br><span class="line">cd /export/server/spark/sbin</span><br><span class="line">./start-master.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 sbin]# jps</span><br><span class="line">7393 HistoryServer</span><br><span class="line">6034 NameNode</span><br><span class="line">7012 NodeManager</span><br><span class="line">9749 Worker</span><br><span class="line">9639 Master</span><br><span class="line">2603 DataNode</span><br><span class="line">6699 ResourceManager</span><br><span class="line">9805 Jps</span><br><span class="line">4911 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">(base) [root@node2 sbin]# jps</span><br><span class="line">2736 QuorumPeerMain</span><br><span class="line">4130 Jps</span><br><span class="line">4071 Master</span><br><span class="line">3480 NodeManager</span><br><span class="line">3386 SecondaryNameNode</span><br><span class="line">3263 DataNode</span><br><span class="line">3711 Worker</span><br></pre></td></tr></table></figure>

<h4 id="4-访问WebUI"><a href="#4-访问WebUI" class="headerlink" title="4.访问WebUI"></a>4.访问WebUI</h4><p><figure class="figure"><img src="/2022/05/22/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/7.png" alt="7.png"><figcaption class="figure__caption">7.png</figcaption></figure></p>
<p><figure class="figure"><img src="/2022/05/22/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/8.png" alt="8.png"><figcaption class="figure__caption">8.png</figcaption></figure></p>
<h4 id="5-为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机"><a href="#5-为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机" class="headerlink" title="5.为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机"></a>5.为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# kill -9 9639</span><br><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">7393 HistoryServer</span><br><span class="line">6034 NameNode</span><br><span class="line">7012 NodeManager</span><br><span class="line">9940 Jps</span><br><span class="line">9749 Worker</span><br><span class="line">2603 DataNode</span><br><span class="line">6699 ResourceManager</span><br><span class="line">4911 QuorumPeerMain</span><br></pre></td></tr></table></figure>

<h4 id="6-此刻我们访问备用master（node2）的WebUI"><a href="#6-此刻我们访问备用master（node2）的WebUI" class="headerlink" title="6.此刻我们访问备用master（node2）的WebUI"></a>6.此刻我们访问备用master（node2）的WebUI</h4><p><figure class="figure"><img src="/2022/05/22/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/9.png" alt="9.png"><figcaption class="figure__caption">9.png</figcaption></figure></p>
<blockquote>
<p>由此可得出结论</p>
<blockquote>
<p>在Spark-Standalone-HA模式下，主备切换不会出现任何错误，在其中的一个master宕机后，也会有备用的master继续工作。</p>
</blockquote>
</blockquote>
<hr>
<h3 id="二、Spark-On-YARN模式"><a href="#二、Spark-On-YARN模式" class="headerlink" title="二、Spark On YARN模式"></a>二、Spark On YARN模式</h3><p>*** spark on yarn架构有两种模式，分为Yarn-client模式和Yarn-cluster模式***</p>
<h4 id="1-配置spark-env-sh"><a href="#1-配置spark-env-sh" class="headerlink" title="1. 配置spark-env.sh"></a>1. 配置spark-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#添加</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="2-启动pyspark-–master-yarn-并测试"><a href="#2-启动pyspark-–master-yarn-并测试" class="headerlink" title="2.启动pyspark –master yarn,并测试"></a>2.启动pyspark –master yarn,并测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/bin</span><br><span class="line"></span><br><span class="line">Python 3.8.12 (default, Oct 12 2021, 13:49:34) </span><br><span class="line">[GCC 7.5.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"></span><br><span class="line">22/05/28 02:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/05/28 02:06:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.12 (default, Oct 12 2021 13:49:34)</span><br><span class="line">Spark context Web UI available at http://node1:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = yarn, app id = application_1653500749142_0001).</span><br><span class="line">SparkSession available as &#x27;spark&#x27;.</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; sc.parallelize([1,2,3,4,5]).map(lambda x: x*10).collect()</span></span><br><span class="line">[10, 20, 30, 40, 50] </span><br></pre></td></tr></table></figure>

<p>访问node1:8088查看：</p>
<p><figure class="figure"><img src="/2022/05/22/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/10.png" alt="10.png"><figcaption class="figure__caption">10.png</figcaption></figure></p>
<h4 id="4-验证client模式"><a href="#4-验证client模式" class="headerlink" title="4.验证client模式"></a>4.验证client模式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# /export/server/spark/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 3</span><br><span class="line"></span><br><span class="line">22/05/28 02:22:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/05/28 02:22:19 WARN Utils: Service &#x27;SparkUI&#x27; could not bind on port 4040. Attempting port 4041.</span><br><span class="line">22/05/28 02:22:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">Pi is roughly 3.146480</span><br></pre></td></tr></table></figure>

<h4 id="5-验证cluster模式"><a href="#5-验证cluster模式" class="headerlink" title="5.验证cluster模式"></a>5.验证cluster模式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# /export/server/spark/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 3</span><br><span class="line"></span><br><span class="line">22/05/28 02:24:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/05/28 02:24:01 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️：为什么两者的输出一个有结果一个没有结果？</p>
<blockquote>
<p>是因为clint（客户端）模式和cluster模式是有区别的：</p>
<p>client模式：Driver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）</p>
<p>cluster模式：Driver程序在YARN中运行，Driver所在的机器是随机的，应用的运行结果不能在客户端显示只能通过yarn查看，所以最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql）而非stdout输出的应用程序，客户端的终端显示的仅是作为YARN的job的简单运行状况。</p>
</blockquote>
</blockquote>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/15/hello-world/">Hello World</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-15</span><br />
		
		
	</div>

	

	
		<p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/03/10/github基础/">Github基础</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-03-10</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/%E6%9A%82%E6%97%B6%E5%81%9C%E6%9B%B4/" rel="tag">暂时停更</a>
			</span>
		
	</div>

	

	
		<h1 id="Git-基础操作-（4-1）"><a href="#Git-基础操作-（4-1）" class="headerlink" title="Git 基础操作 （4-1）"></a><strong>Git 基础操作 （4-1）</strong></h1><ul>
<li><h3 id="git-init-——–初始化仓库"><a href="#git-init-——–初始化仓库" class="headerlink" title="git init ——–初始化仓库"></a>git init ——–初始化仓库</h3></li>
</ul>
<p>在使用Git进行版本管理，必须先出实话仓库。出实话之前先创建一个目录（git-tutorial）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir git-tutorial</span><br><span class="line">$ cd git-tutorial</span><br><span class="line">$ git init</span><br><span class="line">hint: Using &#x27;master&#x27; as the name for the initial branch. This default branch name</span><br><span class="line">hint: is subject to change. To configure the initial branch name to use in all</span><br><span class="line">hint: of your new repositories, which will suppress this warning, call:</span><br><span class="line">hint: </span><br><span class="line">hint: 	git config --global init.defaultBranch &lt;name&gt;</span><br><span class="line">hint: </span><br><span class="line">hint: Names commonly chosen instead of &#x27;master&#x27; are &#x27;main&#x27;, &#x27;trunk&#x27; and</span><br><span class="line">hint: &#x27;development&#x27;. The just-created branch can be renamed via this command:</span><br><span class="line">hint: </span><br><span class="line">hint: 	git branch -m &lt;name&gt; </span><br></pre></td></tr></table></figure>

<p>代码执行成后，目录会生成.git目录（储存管理当前目录内容所需的数据仓库）</p>
<ul>
<li><h3 id="git-status-——-查看仓库状态"><a href="#git-status-——-查看仓库状态" class="headerlink" title="git status ——-查看仓库状态"></a>git status ——-查看仓库状态</h3></li>
</ul>
<h5 id="git-status命令用于显示Git仓库的状态。"><a href="#git-status命令用于显示Git仓库的状态。" class="headerlink" title="git status命令用于显示Git仓库的状态。"></a><strong>git status命令用于显示Git仓库的状态。</strong></h5><p>因工作树和仓库被操作的过程中，状态会不断发生变化。所以需要git status 命令查看当前状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git ststua</span><br><span class="line">On branch master        #当前处于master分支</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">nothing to commit (create/copy files and use &quot;git add&quot; to track)</span><br></pre></td></tr></table></figure>

<p>所谓提交（commit）：是指“ 记录工作数中所有文件的当前状态 ”</p>
<p>因为我们第一次操作没有可提交内容（没有记录任何文件的任何状态），所以我们建立README.md文件作为对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ touch README.md </span><br><span class="line">$ git status </span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line">	README.md</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</span><br></pre></td></tr></table></figure>

<p>可以看到在Untracked files中显示了README.md文件。以此类推只要对Git进行操作 <strong>git status</strong>显示的结果就会发变化。</p>
<ul>
<li><h3 id="git-add-———先暂时区中添加文件"><a href="#git-add-———先暂时区中添加文件" class="headerlink" title="git add ———先暂时区中添加文件"></a>git add ———先暂时区中添加文件</h3></li>
</ul>
<p> 如果仅仅用Git仓库的工作树创建了文件，那么该文件并不会被记入git仓库的版本管理对象中。所以当使用<strong>git status</strong>时会显示在Untracked files中。</p>
<p>若想将其成为Git仓库的管理对象，解决办法是使用 <strong>git add</strong>命令将其加入暂存区（stage or index） ==暂存区是提交之前的一个临时区域==</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git add README.md </span><br><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage)</span><br><span class="line">	new file:   README.md</span><br></pre></td></tr></table></figure>

<p>将README.md加入暂存区后，<strong>git status</strong>可看出README.md文件在changes to be committed 中。</p>
<ul>
<li><h3 id="git-commit-———保存仓库的历史记录"><a href="#git-commit-———保存仓库的历史记录" class="headerlink" title="git commit ———保存仓库的历史记录"></a>git commit ———保存仓库的历史记录</h3><p><strong>git commit</strong>命令可将当前出存在暂存区的文件保存到仓库的历史记录中。通过这些，我们可以在工作树中福源文件。</p>
<ul>
<li><p>记述一行提交信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;First commit&quot;</span><br><span class="line">[master (root-commit) 335488e] First commit</span><br><span class="line"> 1 file changed, 0 insertions(+), 0 deletions(-)</span><br><span class="line"> create mode 100644 README.md</span><br></pre></td></tr></table></figure>

<p>-m后的“First commit” 是对提交的概述.</p>
</li>
<li><p>记述详细提交信息</p>
<p>若想记录更详细，则不加 -m,执行后直接编辑即可</p>
<p>● 第一行：用一行文字简述提交的更改内容</p>
<p>● 第二行：空行</p>
<p>● 第三行以后：记述更改的原因和详细内容</p>
</li>
<li><p>中止提交</p>
<p>若在编译器启动后想终止，则将提交信息留空并关闭编译器即可</p>
</li>
</ul>
</li>
<li><h3 id="git-log-——–查看提交日志"><a href="#git-log-——–查看提交日志" class="headerlink" title="git log ——–查看提交日志"></a>git log ——–查看提交日志</h3><p><strong>git log</strong>可以查看以往仓库中提交的日志。如：什么人在什么时候进行了提交或合并，以及操作前后有怎样的差别。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git log </span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 09:47:09 2022 -0400</span><br><span class="line"></span><br><span class="line">    First commit</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>git log –pretty=short</strong>不显示Date，方便把握多个提交。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git log </span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line"></span><br><span class="line">    First commit</span><br></pre></td></tr></table></figure></li>
<li><p>特定查找摸个文件</p>
<p><code>$ git log READMR.md</code></p>
</li>
<li><p>显示文件的改动</p>
<p>若想看提交所带来的改动怎加上 -p </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ git log -p</span><br><span class="line">$ git log -p README.md</span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 09:47:09 2022 -0400</span><br><span class="line"></span><br><span class="line">    First commit</span><br><span class="line"></span><br><span class="line">diff --git a/README.md b/README.md</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000..e69de29</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<ul>
<li><h3 id="git-diff-——–查看更改前后的差别"><a href="#git-diff-——–查看更改前后的差别" class="headerlink" title="git diff ——–查看更改前后的差别"></a>git diff ——–查看更改前后的差别</h3><p>先vim README.md</p>
<p><code># Git教程</code></p>
<ul>
<li><p>查看工作树和暂存区的差别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git diff</span><br><span class="line">diff --git a/README.md b/README.md</span><br><span class="line">index e69de29..981aef2 100644</span><br><span class="line">--- a/README.md</span><br><span class="line">+++ b/README.md</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+#Git教程</span><br></pre></td></tr></table></figure>

<p>在最后一行出现的+（-）表示先添加的行（被删除）</p>
<p>用<strong>git add</strong>将md文件加入暂存区</p>
<p><code>$ git add READMD.md</code></p>
<ul>
<li><h4 id="查看工作树和最新提交的差别"><a href="#查看工作树和最新提交的差别" class="headerlink" title="查看工作树和最新提交的差别"></a>查看工作树和最新提交的差别</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git diff HEAD</span><br><span class="line">diff --git a/README.md b/README.md</span><br><span class="line">index e69de29..981aef2 100644</span><br><span class="line">--- a/README.md</span><br><span class="line">+++ b/README.md</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+#Git教程</span><br></pre></td></tr></table></figure>

<p><strong>养成好习惯：执行git commit 前先执行 git diff HEAD</strong></p>
<p>对比后进行提交 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;Add index&quot;</span><br><span class="line">[master fe4c8d9] Add index</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>

<p>保险起见再查看下一觉日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ git log</span><br><span class="line">commit fe4c8d999e632799390fc35ca8d9529e90940014 (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 10:02:02 2022 -0400</span><br><span class="line"></span><br><span class="line">    Add index</span><br><span class="line"></span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 09:47:09 2022 -0400</span><br><span class="line"></span><br><span class="line">    First commit</span><br></pre></td></tr></table></figure>

<p>看到两个commit表示成功    </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="分支的操作（4-2）"><a href="#分支的操作（4-2）" class="headerlink" title="分支的操作（4-2）"></a>分支的操作（4-2）</h1><ul>
<li><h3 id="git-branch-——–显示分支—-览表"><a href="#git-branch-——–显示分支—-览表" class="headerlink" title="git branch ——–显示分支—-览表"></a>git branch ——–显示分支—-览表</h3><p><strong>git branch</strong>显示当前所在分支</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* master</span><br></pre></td></tr></table></figure></li>
<li><h3 id="git-checkout-b-———-创建、切换分支"><a href="#git-checkout-b-———-创建、切换分支" class="headerlink" title="git checkout -b ———-创建、切换分支"></a>git checkout -b ———-创建、切换分支</h3><ul>
<li><p>切换到feature -A分支并进行提交(两种方法)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git branch feature-A</span><br><span class="line">$ git checkout -b feature-A</span><br><span class="line">Switched to a new branch &#x27;feature-A&#x27;</span><br><span class="line">$ git branch</span><br><span class="line">* feature-A</span><br><span class="line">  master</span><br></pre></td></tr></table></figure>

<p><strong>不断对一个分支进行提交的操作被称为“培育分支”</strong></p>
<p>œ添加到feature-A分支中</p>
<blockquote>
<p>$ vim README.md</p>
<blockquote>
<p>#Git教程</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;Add feature-A&quot;</span><br><span class="line">[feature-A e5033d6] Add feature-A</span><br><span class="line"> 1 file changed, 2 insertions(+)</span><br></pre></td></tr></table></figure></blockquote>
</li>
<li><p>切换到master分支</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br></pre></td></tr></table></figure>

<p>切换到master分支中看到README.md中的内容不变。因为feature-A不会影响master。</p>
</li>
<li><p>切换回上一个分支</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -</span><br><span class="line">Switched to branch &#x27;feature-A&#x27;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>最近只在学习github，一直没有整理，没有一直更新，不久就会更新 ！😭</p>

	

	

</article>






	<span class="different-posts">📕 end of posts 📕</span>


	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2022 notrip | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
