<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>Hexo</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
<meta name="generator" content="Hexo 5.4.2"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Hexo</a></h1>
		<h2 class="header__subtitle"></h2>
	</header>

	<main>
		



	<article>
	
		<h1><a href="/2022/05/22/Spark基础环境配置/">Spark基础环境配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-22</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/" rel="tag">Spark基础环境配置</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><ul>
<li>Mac 用户注意需要注意，虚拟机使用VMware Fusion，镜像为CentOS7.</li>
</ul>
<hr>
<h2 id="一、基础配置"><a href="#一、基础配置" class="headerlink" title="一、基础配置"></a>一、基础配置</h2><h4 id="1-配置网络适配"><a href="#1-配置网络适配" class="headerlink" title="1.配置网络适配"></a><strong>1.配置网络适配</strong></h4><p>Mac用户只需点击VMware Fusion &gt; 偏好设置 &gt; 网络，并点击左下角“ + ”即可添加虚拟机上所用的子网ip（例：192.168.138.0）。</p>
<p><figure class="figure"><img src="/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/1.png" alt="1.png"><figcaption class="figure__caption">1.png</figcaption></figure></p>
<p><em>注：Mac用户若省略此处步骤可能会导致时间的推移而导致ping不动网络（血的教训😭）</em></p>
<ul>
<li><p>分别修改三台虚拟机的主机名</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node1</span></span><br><span class="line">echo &quot;node1&quot; &gt;/etc/hostname</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node2</span></span><br><span class="line">echo &quot;node2&quot; &gt;/etc/hostname</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node3</span></span><br><span class="line">echo &quot;node3&quot; &gt;/etc/hostname</span><br></pre></td></tr></table></figure></li>
<li><p>更该网络配置 （node1 node2 node3）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-ens33(node2,node3中的IPADDR只需在151上递加即可)</span><br><span class="line"></span><br><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;# 配置BOOTPROTO=static：表示静态路由协议，可以保持IP固定</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;7815751b-505d-4ae2-b2d4-aa39591dc6aa&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;# 配置ONBOOT=yes:表示启动这块网卡</span><br><span class="line">IPADDR=&quot;192.168.138.151&quot;# 配置IPADDR:表示虚拟机的IP地址，这里设置的IP地址要与前面IP映射配置时的IP地址一致，否则无法通过主机名找到对应IP;</span><br><span class="line">PREFIX=&quot;24&quot;</span><br><span class="line">GATEWAY=&quot;192.168.138.2&quot;# 配置GATEWAY:表示虚拟机网关,通常都是将IP地址最后一个位数变为2;</span><br><span class="line">DNS1=&quot;192.168.138.2&quot;# 配置DNS1:表示域名解析器，此处采用Google提供的免费DNS服务器88.8.8(也可以设置为PC端电脑对应的DNS)。</span><br><span class="line">DOMAIN=&quot;114.114.114.114&quot;</span><br><span class="line">IPV6_PRIVACY=&quot;no&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>更改完成后对三台虚拟机进行重启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shutdown -r 0</span><br></pre></td></tr></table></figure></li>
<li><p>重启成功后进行一次连通外网的测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ping baidu.com -c3</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="2-修改host映射"><a href="#2-修改host映射" class="headerlink" title="2.修改host映射"></a>2.修改host映射</h4><p>修改/etc/hosts映射文件，为的是将来对相关联的虚拟机操作更加便捷，其中添加相互对应的IP地址和主机名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.138.151 node1 node1.itcast.cn</span><br><span class="line">192.168.138.152 node2 node2.itcast.cn</span><br><span class="line">192.168.138.153 node3 node3.itcast.cn                                 </span><br></pre></td></tr></table></figure>

<p>新手须知：推出并保存的方式可为 ‘esc + x‘，’esc + shift+zz‘</p>
<h4 id="3-集群的时间同步"><a href="#3-集群的时间同步" class="headerlink" title="3.集群的时间同步"></a>3.集群的时间同步</h4><p>我们有的时候在使用虚拟机时遇到数据误差的时候，总是不清楚哪里出错，原因就在可能没有进行时间同步。在整个集群当中，时间同步充当十分重要的角色。不要存在侥幸心理，如若因为时间不同步造成了损失，即使是进行了数据备份，那么你可能无法在正确的时间将正确的数据进行备份。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp5.aliyun.com</span><br></pre></td></tr></table></figure>

<p>建议：在每次执行poweroff之后的再启动后执行。</p>
<h4 id="4-SSH免密钥登陆"><a href="#4-SSH免密钥登陆" class="headerlink" title="4. SSH免密钥登陆"></a>4. SSH免密钥登陆</h4><ul>
<li><p>在node1上生成公钥私钥</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br><span class="line"></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/root/.ssh/id_rsa): </span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /root/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /root/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br><span class="line">SHA256:QUAgFH5KBc/Erlf1JWSBbKeEepPJqMBqpWbc02/uFj8 root@master</span><br><span class="line">The key&#x27;s randomart image is:</span><br><span class="line">+---[RSA 2048]----+</span><br><span class="line">| .=++oo+.o+.     |</span><br><span class="line">| . *. ..*.o .    |</span><br><span class="line">|. o.++ *.+ o     |</span><br><span class="line">|.o ++ B ...      |</span><br><span class="line">|o.=o.o .S        |</span><br><span class="line">|.*oo.. .         |</span><br><span class="line">|+  .. . o        |</span><br><span class="line">|       + E       |</span><br><span class="line">|      =o  .      |</span><br><span class="line">+----[SHA256]-----+</span><br></pre></td></tr></table></figure></li>
<li><p>在node1上配置免密登陆node1 node2 node3</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id master</span><br><span class="line">ssh-copy-id slave1</span><br><span class="line">ssh-copy-id slave2</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h2 id="二、安装配置JDK"><a href="#二、安装配置JDK" class="headerlink" title="二、安装配置JDK"></a>二、安装配置JDK</h2><h4 id="1-创建编译环境软件所需的根目录"><a href="#1-创建编译环境软件所需的根目录" class="headerlink" title="1.创建编译环境软件所需的根目录"></a>1.创建编译环境软件所需的根目录</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/server</span><br></pre></td></tr></table></figure>

<h4 id="2-安装JDK1-8-（首先将-jdk-8u241-linux-x64-tar-gz压缩包上传到-export-server目录，并解压）"><a href="#2-安装JDK1-8-（首先将-jdk-8u241-linux-x64-tar-gz压缩包上传到-export-server目录，并解压）" class="headerlink" title="2.安装JDK1.8 （首先将 jdk-8u241-linux-x64.tar.gz压缩包上传到/export/server目录，并解压）"></a>2.安装JDK1.8 （首先将 jdk-8u241-linux-x64.tar.gz压缩包上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u241-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="3-配置全局环境变量"><a href="#3-配置全局环境变量" class="headerlink" title="3.配置全局环境变量"></a>3.配置全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br></pre></td></tr></table></figure>

<h4 id="4-加载环境变量"><a href="#4-加载环境变量" class="headerlink" title="4.加载环境变量"></a>4.加载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="5-查看java版本"><a href="#5-查看java版本" class="headerlink" title="5.查看java版本"></a>5.查看java版本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">(base) [root@node1 ~]# java -version</span><br><span class="line">java version &quot;1.8.0_241&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_241-b07)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode)</span><br></pre></td></tr></table></figure>

<p>出现以下内容则代表安装成功</p>
<h4 id="6-将java文件从node1中传输到node2-node3中，传输完成后，配置方法参考-3-4"><a href="#6-将java文件从node1中传输到node2-node3中，传输完成后，配置方法参考-3-4" class="headerlink" title="6.将java文件从node1中传输到node2 node3中，传输完成后，配置方法参考 3. 4."></a>6.将java文件从node1中传输到node2 node3中，传输完成后，配置方法参考 3. 4.</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>

<h4 id="7-在三台主机分别创建软连接（为了方便）"><a href="#7-在三台主机分别创建软连接（为了方便）" class="headerlink" title="7. 在三台主机分别创建软连接（为了方便）"></a>7. 在三台主机分别创建软连接（为了方便）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">ln -s jdk1.8.0_241/jdk</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="三、安装配置Hadoop"><a href="#三、安装配置Hadoop" class="headerlink" title="三、安装配置Hadoop"></a>三、安装配置Hadoop</h2><h4 id="1-安装Hadoop-（首先将hadoop-3-3-0-Centos7-with-wnappy-tar-gz压缩包上传到-export-server目录，并解压）"><a href="#1-安装Hadoop-（首先将hadoop-3-3-0-Centos7-with-wnappy-tar-gz压缩包上传到-export-server目录，并解压）" class="headerlink" title="1.安装Hadoop （首先将hadoop-3.3.0-Centos7-with-wnappy.tar.gz压缩包上传到/export/server目录，并解压）"></a>1.安装Hadoop （首先将hadoop-3.3.0-Centos7-with-wnappy.tar.gz压缩包上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.0-Centos7-with-wnappy.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="2-创建软连接"><a href="#2-创建软连接" class="headerlink" title="2.创建软连接"></a>2.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s jhadoop-3.3.0/ hadoop</span><br></pre></td></tr></table></figure>

<h4 id="3-修改配置文件"><a href="#3-修改配置文件" class="headerlink" title="3.修改配置文件"></a>3.修改配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>

<ul>
<li><p>修改Hadoop-env.sh文件 </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim hadoop-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">末尾添加</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line"></span><br><span class="line">export HDFS_NAMENODE_USER=root</span><br><span class="line">export HDFS_DATANODE_USER=root</span><br><span class="line">export HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line">export YARN_RESOURCEMANAGER_USER=root</span><br><span class="line">export YARN_NODEMANAGER_USER=root</span><br></pre></td></tr></table></figure></li>
<li><p>修改core-site.xml文件</p>
<ul>
<li><p>设置默认系统</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://node1:8020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置hadoop本地保存数据路径</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置HDFS web UI用户身份</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>整合hive用户代理设置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置文件系统垃圾桶保存时间</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>配置mapred-site.xml</p>
<ul>
<li><p>设置MR程序默认运行模式：yarn集群模式 local本地模式</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置MR程序历史服务地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置MR程序历史服务器web端地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;node1:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>修改yarn-site.xml</p>
<ul>
<li><p>设置YARN集群主角色运行机器位置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;node1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>是否将对容器实施物理内存限制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>设置yarn历史服务器地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
<li><p>历史日志保存的时间 7天</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="4-将Hadoop文件从node1中传输到node2-node3中"><a href="#4-将Hadoop文件从node1中传输到node2-node3中" class="headerlink" title="4. 将Hadoop文件从node1中传输到node2 node3中"></a>4. 将Hadoop文件从node1中传输到node2 node3中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/hadoop/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/hadoop/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>

<h4 id="5-三台主机分别配置全局变量"><a href="#5-三台主机分别配置全局变量" class="headerlink" title="5. 三台主机分别配置全局变量"></a>5. 三台主机分别配置全局变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<h4 id="6-重新加载全局环境变量"><a href="#6-重新加载全局环境变量" class="headerlink" title="6.重新加载全局环境变量"></a>6.重新加载全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="7-Hadoop集群启动前需要进行格式化"><a href="#7-Hadoop集群启动前需要进行格式化" class="headerlink" title="7.Hadoop集群启动前需要进行格式化"></a>7.Hadoop集群启动前需要进行格式化</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h4 id="8-启动hadoop集群"><a href="#8-启动hadoop集群" class="headerlink" title="8.启动hadoop集群"></a>8.启动hadoop集群</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/hadoop/bin</span><br><span class="line"></span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure>

<h4 id="9-查看当前所有进程服务"><a href="#9-查看当前所有进程服务" class="headerlink" title="9.查看当前所有进程服务"></a>9.查看当前所有进程服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">3040 ResourceManager</span><br><span class="line">3713 Jps</span><br><span class="line">3240 NodeManager</span><br><span class="line">2603 DataNode</span><br><span class="line">2335 NameNode</span><br><span class="line"></span><br><span class="line">(base) [root@node2 ~]# jps</span><br><span class="line">2322 Jps</span><br><span class="line">2101 SecondaryNameNode</span><br><span class="line">2199 NodeManager</span><br><span class="line">1978 DataNode</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line"></span><br><span class="line">(base) [root@node3 ~]# jps</span><br><span class="line">2225 Jps</span><br><span class="line">1972 DataNode</span><br><span class="line">2101 NodeManager</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure>

<h4 id="10-进入hdfs-yarn-web-UI页面"><a href="#10-进入hdfs-yarn-web-UI页面" class="headerlink" title="10.进入hdfs yarn web UI页面"></a>10.进入hdfs yarn web UI页面</h4><p><figure class="figure"><img src="/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/2.png" alt="2.png"><figcaption class="figure__caption">2.png</figcaption></figure></p>
<p><figure class="figure"><img src="/2022/05/22/Spark%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/3.png" alt="3.png"><figcaption class="figure__caption">3.png</figcaption></figure></p>
<hr>
<h2 id="四、安装配置zookeeper"><a href="#四、安装配置zookeeper" class="headerlink" title="四、安装配置zookeeper"></a>四、安装配置zookeeper</h2><h4 id="1-安装zookeeper-（首先将zookeeper-3-4-10-tar-gz压缩包上传到-export-server目录，并解压）"><a href="#1-安装zookeeper-（首先将zookeeper-3-4-10-tar-gz压缩包上传到-export-server目录，并解压）" class="headerlink" title="1.安装zookeeper （首先将zookeeper-3.4.10.tar.gz压缩包上传到/export/server目录，并解压）"></a>1.安装zookeeper （首先将zookeeper-3.4.10.tar.gz压缩包上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz</span><br></pre></td></tr></table></figure>

<h4 id="2-创建软连接-1"><a href="#2-创建软连接-1" class="headerlink" title="2.创建软连接"></a>2.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure>

<h4 id="3-修改配置文件-1"><a href="#3-修改配置文件-1" class="headerlink" title="3.修改配置文件"></a>3.修改配置文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/conf/ </span><br></pre></td></tr></table></figure>

<ul>
<li><p>将 zoo_sample.cfg 文件复制为新文件 zoo.cfg </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure></li>
<li><p>配置zoo.cfg文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">vim zoo.cfg</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">Zookeeper的数据存放目录</span></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">集群中服务器地址</span></span><br><span class="line">server.1=node1:2888:3888</span><br><span class="line">server.2=node2:2888:3888</span><br><span class="line">server.3=node3:2888:3888</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="4-进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-1-写入进去"><a href="#4-进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-1-写入进去" class="headerlink" title="4.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去"></a>4.进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 1 写入进去</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/zkdata</span><br><span class="line"></span><br><span class="line">touch myid</span><br><span class="line"></span><br><span class="line">echo &#x27;1&#x27; &gt; myid</span><br></pre></td></tr></table></figure>

<h4 id="5-将zookeeper文件从node1中传输到node2-node3中"><a href="#5-将zookeeper文件从node1中传输到node2-node3中" class="headerlink" title="5.将zookeeper文件从node1中传输到node2 node3中"></a>5.将zookeeper文件从node1中传输到node2 node3中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/zookeeper-3.4.10/ node2:$PWD</span><br><span class="line">scp -r /export/server/zookeeper-3.4.10/ node3:$PWD</span><br></pre></td></tr></table></figure>

<h4 id="6-创建软连接"><a href="#6-创建软连接" class="headerlink" title="6.创建软连接"></a>6.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s zookeeper-3.4.10/ zookeeper</span><br></pre></td></tr></table></figure>

<h4 id="7-node2-node3分别进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-2-，3-写入进去"><a href="#7-node2-node3分别进入-export-server-zookeeper-zkdatas-目录在此目录下创建-myid-文件，将-2-，3-写入进去" class="headerlink" title="7.node2 node3分别进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 2 ，3 写入进去"></a>7.node2 node3分别进入 /export/server/zookeeper/zkdatas 目录在此目录下创建 myid 文件，将 2 ，3 写入进去</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/zookeeper/zkdatas/</span><br><span class="line"></span><br><span class="line">[root@node2 zkdatas]# vim myid </span><br><span class="line">[root@node2 zkdatas]# more myid </span><br><span class="line">2</span><br><span class="line"></span><br><span class="line">[root@node3 zkdatas]# vim myid </span><br><span class="line">[root@node3 zkdatas]# more myid </span><br><span class="line">3</span><br></pre></td></tr></table></figure>

<h4 id="8-配置全局环境变量"><a href="#8-配置全局环境变量" class="headerlink" title="8.配置全局环境变量"></a>8.配置全局环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">zookeeper 环境变量</span></span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br></pre></td></tr></table></figure>

<h4 id="9-重新加载环境变量"><a href="#9-重新加载环境变量" class="headerlink" title="9.重新加载环境变量"></a>9.重新加载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="10-启动zookeeper服务"><a href="#10-启动zookeeper服务" class="headerlink" title="10.启动zookeeper服务"></a>10.启动zookeeper服务</h4><ul>
<li><p>普通启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd  /export/server/zookeeper-3.4.10/bin </span><br><span class="line"></span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure></li>
<li><p>编写脚本进行一键启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">cd /bin</span><br><span class="line"></span><br><span class="line">vim zkall.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">if [ $# -eq 0 ]</span><br><span class="line">then</span><br><span class="line">echo &quot;please input param:start stop&quot;</span><br><span class="line">else</span><br><span class="line">if [ $1 = start  ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ing node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = stop ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;$&#123;1&#125;ping node$&#123;i&#125;&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $1 = status ]</span><br><span class="line">then</span><br><span class="line">for i in &#123;1..3&#125;</span><br><span class="line">do</span><br><span class="line">echo &quot;node$&#123;i&#125; status:&quot;</span><br><span class="line">ssh node$&#123;i&#125; &quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot;</span><br><span class="line">done</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<ul>
<li><p>给编写的脚本赋予可执行权限</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x zkall.sh</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>注：启动时，无需进入任意文件目录下，直接执行zkall.sh start 即可</strong></p>
</li>
</ul>
<h4 id="11-查看进程"><a href="#11-查看进程" class="headerlink" title="11.查看进程"></a>11.查看进程</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">4957 Jps</span><br><span class="line">4911 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">(base) [root@node2 ~]# jps</span><br><span class="line">2736 QuorumPeerMain</span><br><span class="line">2795 Jps</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br><span class="line"></span><br><span class="line">(base) [root@node3 ~]# jps</span><br><span class="line">2576 Jps</span><br><span class="line">2533 QuorumPeerMain</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure>


	

	

</article>




	<article>
	
		<h1><a href="/2022/05/22/Spark-local-stand-alone配置/">Spark local&amp; stand-alone配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-22</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/" rel="tag">Spark local& stand-alone配置</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h2 id="安装配置Spark"><a href="#安装配置Spark" class="headerlink" title="安装配置Spark"></a>安装配置Spark</h2><p><em><strong>Spark是专为大规模数据处理而设计的快速通用的计算引擎，其提供了一个全面、统一的框架用于管理各种不同性质的数据集和数据源的大数据处理的需求，大数据开发需掌握Spark基础、SparkJob、Spark RDD、spark job部署与资源分配、Spark shuffle、Spark内存管理、Spark广播变量、Spark SQL、Spark Streaming以及Spark ML等相关知识。</strong></em></p>
<hr>
<h2 id="一、Spark-local模式"><a href="#一、Spark-local模式" class="headerlink" title="一、Spark-local模式"></a>一、Spark-local模式</h2><p><em><strong>local模式是以一个单独的进程，通过其内部的多个线程来模拟整个spark</strong></em></p>
<h4 id="1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式只需在node1上安装即可）"><a href="#1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式只需在node1上安装即可）" class="headerlink" title="1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式只需在node1上安装即可）"></a>1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式只需在node1上安装即可）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">执行过程中需注意：</span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现内容选 <span class="built_in">yes</span></span></span><br><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; <span class="built_in">yes</span></span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现添加路径：/export/server/anaconda3</span></span><br><span class="line">...</span><br><span class="line">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3</span><br><span class="line">PREFIX=/export/server/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>安装成功后，现推出，再重新登录终端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br><span class="line"></span><br><span class="line">(base) [root@node1 ~]# </span><br></pre></td></tr></table></figure>

<p>⚠️：出现（base）则代表安装成功！</p>
<h4 id="2-创建基于python3-8虚拟环境的pyspark"><a href="#2-创建基于python3-8虚拟环境的pyspark" class="headerlink" title="2.创建基于python3.8虚拟环境的pyspark"></a>2.创建基于python3.8虚拟环境的pyspark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 </span><br></pre></td></tr></table></figure>

<h4 id="3-切换到pyspark虚拟环境内"><a href="#3-切换到pyspark虚拟环境内" class="headerlink" title="3.切换到pyspark虚拟环境内"></a>3.切换到pyspark虚拟环境内</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# conda activate pyspark  </span><br><span class="line">(pyspark) [root@master ~]# </span><br></pre></td></tr></table></figure>

<p>⚠️：出现（pyspark）则代表成功！</p>
<h4 id="4-在虚拟环境内安装所需包"><a href="#4-在虚拟环境内安装所需包" class="headerlink" title="4.在虚拟环境内安装所需包"></a>4.在虚拟环境内安装所需包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<p>⚠️：中间过程可能会出现WARNING 不管即可</p>
<h4 id="5-安装配置Spark（首先将spark-3-2-0-bin-hadoop3-2-tgz上传到-export-server目录，并解压）"><a href="#5-安装配置Spark（首先将spark-3-2-0-bin-hadoop3-2-tgz上传到-export-server目录，并解压）" class="headerlink" title="5. 安装配置Spark（首先将spark-3.2.0-bin-hadoop3.2.tgz上传到/export/server目录，并解压）"></a>5. 安装配置Spark（首先将spark-3.2.0-bin-hadoop3.2.tgz上传到/export/server目录，并解压）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure>

<h4 id="6-创建软连接"><a href="#6-创建软连接" class="headerlink" title="6.创建软连接"></a>6.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<h4 id="7-配置全局环境变量-并重新加载"><a href="#7-配置全局环境变量-并重新加载" class="headerlink" title="7.配置全局环境变量,并重新加载"></a>7.配置全局环境变量,并重新加载</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME							JAVA_HOME: 告知Spark Java的所在路径位置</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME						HADOOP_HOME: 告知Spark  Hadoop的所在路径位置</span></span><br><span class="line">export HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">ZOOKEEPER_HOME</span></span><br><span class="line">export ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line">export PATH=$PATH:$ZOOKEEPER_HOME/bin</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">SPARK_HOME      				SPARK_HOME: 表示Spark安装路径</span></span><br><span class="line">export SPARK_HOME=/export/server/spark</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_CONF_DIR				告知Spark Hadoop的配置文件的路径位置</span></span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">PYSPARK_PYTHON      		PYSPARK_PYTHON: 表示Spark将要运行Python程序, python解释器的位置</span></span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新加载</span></span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk1.8.0_241  </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">PYSPARK_PYTHON</span></span><br><span class="line">export PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重新加载</span></span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure>

<h4 id="8-开启"><a href="#8-开启" class="headerlink" title="8.开启"></a>8.开启</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line">./pyspark</span><br></pre></td></tr></table></figure>

<p><figure class="figure"><img src="/2022/05/22/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/4.png" alt="4.png"><figcaption class="figure__caption">4.png</figcaption></figure></p>
<h4 id="9-查看web-UI界面"><a href="#9-查看web-UI界面" class="headerlink" title="9.查看web UI界面"></a>9.查看web UI界面</h4><h4 id="10-退出"><a href="#10-退出" class="headerlink" title="10.退出"></a>10.退出</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="二、Spark-stand-alone模式"><a href="#二、Spark-stand-alone模式" class="headerlink" title="二、Spark-stand-alone模式"></a>二、Spark-stand-alone模式</h2><p><em><strong>Stand-alone模式中各个角色以独立个体进程的方式存在，一起组成Spark集群</strong></em></p>
<h4 id="1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2-node3全部单独部署）"><a href="#1-安装Anaconda（首先将Anaconda3-2021-05-Linux-x86-64-sh上传到-export-server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2-node3全部单独部署）" class="headerlink" title="1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2 node3全部单独部署）"></a>1.安装Anaconda（首先将Anaconda3-2021.05-Linux-x86_64.sh上传到/export/server目录，并运行）————（此模式需要全部安装，因node1已部署，所以在node2 node3全部单独部署）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">执行过程中需注意：</span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现内容选 <span class="built_in">yes</span></span></span><br><span class="line">Please answer &#x27;yes&#x27; or &#x27;no&#x27;:&#x27;</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; <span class="built_in">yes</span></span></span><br><span class="line">...</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现添加路径：/export/server/anaconda3</span></span><br><span class="line">...</span><br><span class="line">[/root/anaconda3] &gt;&gt;&gt; /export/server/anaconda3</span><br><span class="line">PREFIX=/export/server/anaconda3</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>安装成功后，现推出，再重新登录终端</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">exit</span><br><span class="line"></span><br><span class="line">(base) [root@node1 ~]# </span><br></pre></td></tr></table></figure>

<p>⚠️：node2 node3 分别全部执行！！！</p>
<h4 id="2-把所需的全局环境变量全部传输到node2-node3"><a href="#2-把所需的全局环境变量全部传输到node2-node3" class="headerlink" title="2.把所需的全局环境变量全部传输到node2 node3"></a>2.把所需的全局环境变量全部传输到node2 node3</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">传输 .bashrc :</span></span><br><span class="line">scp ~/.bashrc root@node2:~/</span><br><span class="line">scp ~/.bashrc root@node3:~/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">传输 profile :</span></span><br><span class="line">scp /etc/profile/ root@node2:/etc/</span><br><span class="line">scp /etc/profile/ root@node3:/etc/</span><br></pre></td></tr></table></figure>

<h4 id="3-创建基于python3-8虚拟环境的pyspark"><a href="#3-创建基于python3-8虚拟环境的pyspark" class="headerlink" title="3.创建基于python3.8虚拟环境的pyspark"></a>3.创建基于python3.8虚拟环境的pyspark</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 </span><br></pre></td></tr></table></figure>

<h4 id="4-切换到pyspark虚拟环境内"><a href="#4-切换到pyspark虚拟环境内" class="headerlink" title="4.切换到pyspark虚拟环境内"></a>4.切换到pyspark虚拟环境内</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark </span><br><span class="line"></span><br><span class="line">(base) [root@node2 ~]# conda activate pyspark  </span><br><span class="line">(pyspark) [root@node2 ~]# </span><br></pre></td></tr></table></figure>

<h4 id="5-在虚拟环境内安装所需包"><a href="#5-在虚拟环境内安装所需包" class="headerlink" title="5.在虚拟环境内安装所需包"></a>5.在虚拟环境内安装所需包</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<h4 id="6-配置相关文件"><a href="#6-配置相关文件" class="headerlink" title="6.配置相关文件"></a>6.配置相关文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/conf</span><br></pre></td></tr></table></figure>

<p>⚠️：此操作在node1上执行</p>
<ul>
<li><p>将文件 workers.template 改名为 workers，并配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mv workers.template workers</span><br><span class="line"></span><br><span class="line">vim workers</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">localhost删除，内容追加文末：</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure></li>
<li><p>将文件 spark-env.sh.template 改名为 spark-env.sh，并配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">mv spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"></span><br><span class="line">文末追加内容：</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置JAVA安装目录</span></span></span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 指定spark老大Master的IP和提交任务的通信端口</span></span></span><br><span class="line">export SPARK_MASTER_HOST=node1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知sparkmaster的通讯端口</span></span><br><span class="line">export SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 设置历史服务器 ------将spark上运行的日志记录储存到hdfs中的sparklog中</span></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>此刻打开一个新的shell窗口，然后开启Hadoop</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure>

<p>⚠️：不建议 在启动单独集群时 需要执行单独对应的.sh文件</p>
</li>
<li><p>在HDFS上创建可存放历史日志的文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /sparklog</span><br><span class="line"></span><br><span class="line">hadoop fs -chmod 777 /sparklog</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>将 spark-defaults.conf.template改为 spark-defaults.conf ，并配置文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"></span><br><span class="line">vim spark-defaults.conf</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">开启spark的日期记录功能</span></span><br><span class="line">spark.eventLog.enabled 	true</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志记录的路径</span></span><br><span class="line">spark.eventLog.dir	 hdfs://node1:8020/sparklog/ </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">设置spark日志是否启动压缩</span></span><br><span class="line">spark.eventLog.compress 	true</span><br></pre></td></tr></table></figure></li>
<li><p>将og4j.properties.template改为log4j.properties，并配置文件 (修改第十九行：将INFO改为WARN)</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mv log4j.properties.template log4j.properties</span><br><span class="line"></span><br><span class="line">vim log4j.properties</span><br><span class="line"></span><br><span class="line"> 18 # Set everything to be logged to the console</span><br><span class="line"> 19 log4j.rootCategory=WARN, console</span><br><span class="line"> 20 log4j.appender.console=org.apache.log4j.ConsoleAppender</span><br><span class="line"> 21 log4j.appender.console.target=System.err</span><br><span class="line"> 22 log4j.appender.console.layout=org.apache.log4j.PatternLayout</span><br><span class="line"> 23 log4j.appender.console.layout.ConversionPattern=%d&#123;yy/MM/dd HH:mm:ss&#125; %p %c&#123;1&#125;: %m%n</span><br></pre></td></tr></table></figure>

<p>**此步骤的目的是让输出的日志之输出警告和错误的日志 **   <em>INFO表示输出所有日志</em></p>
<h4 id="7-将Spark文件夹全部传输到node2-node3上"><a href="#7-将Spark文件夹全部传输到node2-node3上" class="headerlink" title="7.将Spark文件夹全部传输到node2 node3上"></a>7.将Spark文件夹全部传输到node2 node3上</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:$PWD</span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:$PWD</span><br></pre></td></tr></table></figure>

<h4 id="8-创建软连接"><a href="#8-创建软连接" class="headerlink" title="8.创建软连接"></a>8.创建软连接</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<h4 id="9-重新加载环境变量"><a href="#9-重新加载环境变量" class="headerlink" title="9.重新加载环境变量"></a>9.重新加载环境变量</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p>⚠️：因为在步骤2中已经将node1中的全局环境变量传输到node2 node3 中，所以执行即可</p>
<h4 id="10-启动history-server服务"><a href="#10-启动history-server服务" class="headerlink" title="10.启动history-server服务"></a>10.启动history-server服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/sbin </span><br><span class="line"></span><br><span class="line">./start-history-server.sh</span><br><span class="line"></span><br><span class="line">(base)[root@node1 sbin]# jps</span><br><span class="line">7393 HistoryServer</span><br><span class="line">6034 NameNode</span><br><span class="line">7012 NodeManager</span><br><span class="line">2603 DataNode</span><br><span class="line">6699 ResourceManager</span><br><span class="line">4911 QuorumPeerMain</span><br><span class="line">7439 Jps</span><br></pre></td></tr></table></figure>

<h4 id="11-查看-webUI"><a href="#11-查看-webUI" class="headerlink" title="11.查看 webUI"></a>11.查看 webUI</h4><p><figure class="figure"><img src="/2022/05/22/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/5.png" alt="5.png"><figcaption class="figure__caption">5.png</figcaption></figure></p>
<h4 id="12-启动-关闭spark-的主从节点-master-worker进程指令"><a href="#12-启动-关闭spark-的主从节点-master-worker进程指令" class="headerlink" title="12.启动/关闭spark 的主从节点 master worker进程指令"></a>12.启动/关闭spark 的主从节点 master worker进程指令</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh 			#启动全部的master和worke    在node1上执行则默认node1为master</span><br><span class="line">sbin/start-master.sh		#只启动执行主机上master</span><br><span class="line">sbin/start-worker.sh		#只启动执行主机上worker</span><br><span class="line">sbin/stop-all.sh				#停止所有</span><br><span class="line">sbin/stop-master.sh			#只停止执行主机上的master</span><br><span class="line">sbin/stop-worker.sh			#只停止执行主机上的worker</span><br></pre></td></tr></table></figure>

<h4 id="13-查看webUI"><a href="#13-查看webUI" class="headerlink" title="13. 查看webUI"></a>13. 查看webUI</h4><p><figure class="figure"><img src="/2022/05/22/Spark-local-stand-alone%E9%85%8D%E7%BD%AE/6.png" alt="6.png"><figcaption class="figure__caption">6.png</figcaption></figure></p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/22/Spark-HA-Yarn配置/">Spark HA &amp; Yarn配置</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-22</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/Spark-HA-Yarn%E9%85%8D%E7%BD%AE/" rel="tag">Spark HA & Yarn配置</a>
			</span>
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h3 id="一、Spark-Standalone-HA模式"><a href="#一、Spark-Standalone-HA模式" class="headerlink" title="一、Spark-Standalone-HA模式"></a>一、Spark-Standalone-HA模式</h3><p><em><strong>Standalone集群是Master-Slaves架构的集群模式，和大部分的Master-Slaves结构集群一样，存在着Master单点故障的问题。 如何解决这个单点故障的问题，Spark提供了两种方案：1.基于文件系统的单点恢复(Single-Node Recovery with Local File System)–只能用于开发或测试环境。2.基于zookeeper的Standby Masters(Standby Masters with ZooKeeper)–可以用于生产环境。</strong></em></p>
<p>⚠️：主机中的zookeeper的版本需要喝spark版本兼容，否则会出现错误。若不兼容则重新下载与之兼容的zookeeper，并删除之前的软连接，再重新配置，最后分配给node2 node3即可。</p>
<h4 id="1-配置修改-spark-env-sh-文件内容"><a href="#1-配置修改-spark-env-sh-文件内容" class="headerlink" title="1.配置修改 spark-env.sh 文件内容"></a>1.配置修改 spark-env.sh 文件内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/conf </span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#为 83 行内容加上注释</span></span></span><br><span class="line">82 # 告知Spark的master运行在哪个机器上</span><br><span class="line">83 # export SPARK_MASTER_HOST=master</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#文末填写以下内容</span></span></span><br><span class="line">105 SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy    .zookeeper.url=node1:2181,node2:2181,node3:2181 -Dspark.deploy.zookeeper.dir    =/spark-ha&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">spark.deploy.recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定Zookeeper的连接地址</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">指定在Zookeeper中注册临时节点的路径</span></span><br></pre></td></tr></table></figure>

<h4 id="2-将spark-env-sh传输到node2-node3上"><a href="#2-将spark-env-sh传输到node2-node3上" class="headerlink" title="2.将spark-env.sh传输到node2 node3上"></a>2.将spark-env.sh传输到node2 node3上</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line"></span><br><span class="line">scp spark-env.sh node3:/export/server/spark/conf/</span><br></pre></td></tr></table></figure>

<h4 id="3-启动集群"><a href="#3-启动集群" class="headerlink" title="3.启动集群"></a>3.启动集群</h4><p>⚠️：在启动集群前需确保相关zookeeper和Hadoop均已启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#首先在node1上启动master 和所有主机上的worker</span></span></span><br><span class="line">cd /export/server/spark/sbin</span><br><span class="line">./start-all.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#再到node2上启动 node2上的master作为备用master</span></span></span><br><span class="line">cd /export/server/spark/sbin</span><br><span class="line">./start-master.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 sbin]# jps</span><br><span class="line">7393 HistoryServer</span><br><span class="line">6034 NameNode</span><br><span class="line">7012 NodeManager</span><br><span class="line">9749 Worker</span><br><span class="line">9639 Master</span><br><span class="line">2603 DataNode</span><br><span class="line">6699 ResourceManager</span><br><span class="line">9805 Jps</span><br><span class="line">4911 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">(base) [root@node2 sbin]# jps</span><br><span class="line">2736 QuorumPeerMain</span><br><span class="line">4130 Jps</span><br><span class="line">4071 Master</span><br><span class="line">3480 NodeManager</span><br><span class="line">3386 SecondaryNameNode</span><br><span class="line">3263 DataNode</span><br><span class="line">3711 Worker</span><br></pre></td></tr></table></figure>

<h4 id="4-访问WebUI"><a href="#4-访问WebUI" class="headerlink" title="4.访问WebUI"></a>4.访问WebUI</h4><p>![markdown](/Users/mojiyang/Desktop/bin/截图库/11.18/截屏2022-05-28 上午1.38.42.png)</p>
<p>![markdown](/Users/mojiyang/Desktop/bin/截图库/11.18/截屏2022-05-28 上午1.40.58.png)</p>
<h4 id="5-为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机"><a href="#5-为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机" class="headerlink" title="5.为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机"></a>5.为验证功能，我们kill掉node1中master的进程号，来模拟node1作为我们正进行操作的主机突然宕机</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# kill -9 9639</span><br><span class="line">(base) [root@node1 ~]# jps</span><br><span class="line">7393 HistoryServer</span><br><span class="line">6034 NameNode</span><br><span class="line">7012 NodeManager</span><br><span class="line">9940 Jps</span><br><span class="line">9749 Worker</span><br><span class="line">2603 DataNode</span><br><span class="line">6699 ResourceManager</span><br><span class="line">4911 QuorumPeerMain</span><br></pre></td></tr></table></figure>

<h4 id="6-此刻我们访问备用master（node2）的WebUI"><a href="#6-此刻我们访问备用master（node2）的WebUI" class="headerlink" title="6.此刻我们访问备用master（node2）的WebUI"></a>6.此刻我们访问备用master（node2）的WebUI</h4><p>![markdown](/Users/mojiyang/Desktop/bin/截图库/11.18/截屏2022-05-28 上午1.47.23.png)</p>
<blockquote>
<p>由此可得出结论</p>
<blockquote>
<p>在Spark-Standalone-HA模式下，主备切换不会出现任何错误，在其中的一个master宕机后，也会有备用的master继续工作。</p>
</blockquote>
</blockquote>
<hr>
<h3 id="二、Spark-On-YARN模式"><a href="#二、Spark-On-YARN模式" class="headerlink" title="二、Spark On YARN模式"></a>二、Spark On YARN模式</h3><p>*** spark on yarn架构有两种模式，分为Yarn-client模式和Yarn-cluster模式***</p>
<h4 id="1-配置spark-env-sh"><a href="#1-配置spark-env-sh" class="headerlink" title="1. 配置spark-env.sh"></a>1. 配置spark-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment">#添加</span></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="2-启动pyspark-–master-yarn-并测试"><a href="#2-启动pyspark-–master-yarn-并测试" class="headerlink" title="2.启动pyspark –master yarn,并测试"></a>2.启动pyspark –master yarn,并测试</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server/spark/bin</span><br><span class="line"></span><br><span class="line">Python 3.8.12 (default, Oct 12 2021, 13:49:34) </span><br><span class="line">[GCC 7.5.0] :: Anaconda, Inc. on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line"></span><br><span class="line">22/05/28 02:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/05/28 02:06:39 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.8.12 (default, Oct 12 2021 13:49:34)</span><br><span class="line">Spark context Web UI available at http://node1:4040</span><br><span class="line">Spark context available as &#x27;sc&#x27; (master = yarn, app id = application_1653500749142_0001).</span><br><span class="line">SparkSession available as &#x27;spark&#x27;.</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">&gt;&gt; sc.parallelize([1,2,3,4,5]).map(lambda x: x*10).collect()</span></span><br><span class="line">[10, 20, 30, 40, 50] </span><br></pre></td></tr></table></figure>

<p>访问node1:8088查看：</p>
<p>![markdown](/Users/mojiyang/Desktop/bin/截图库/11.18/截屏2022-05-28 上午2.14.02.png)</p>
<h4 id="4-验证client模式"><a href="#4-验证client模式" class="headerlink" title="4.验证client模式"></a>4.验证client模式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# /export/server/spark/bin/spark-submit --master yarn --deploy-mode client --driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 3</span><br><span class="line"></span><br><span class="line">22/05/28 02:22:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/05/28 02:22:19 WARN Utils: Service &#x27;SparkUI&#x27; could not bind on port 4040. Attempting port 4041.</span><br><span class="line">22/05/28 02:22:20 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">Pi is roughly 3.146480</span><br></pre></td></tr></table></figure>

<h4 id="5-验证cluster模式"><a href="#5-验证cluster模式" class="headerlink" title="5.验证cluster模式"></a>5.验证cluster模式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(base) [root@node1 ~]# /export/server/spark/bin/spark-submit --master yarn --deploy-mode cluster --driver-memory 512m --executor-memory 512m --num-executors 3 --total-executor-cores 3 /export/server/spark/examples/src/main/python/pi.py 3</span><br><span class="line"></span><br><span class="line">22/05/28 02:24:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">22/05/28 02:24:01 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</span><br><span class="line">You have new mail in /var/spool/mail/root</span><br></pre></td></tr></table></figure>

<blockquote>
<p>⚠️：为什么两者的输出一个有结果一个没有结果？</p>
<blockquote>
<p>是因为clint（客户端）模式和cluster模式是有区别的：</p>
<p>client模式：Driver运行在Client上，应用程序运行结果会在客户端显示，所有适合运行结果有输出的应用程序（如spark-shell）</p>
<p>cluster模式：Driver程序在YARN中运行，Driver所在的机器是随机的，应用的运行结果不能在客户端显示只能通过yarn查看，所以最好运行那些将结果最终保存在外部存储介质（如HDFS、Redis、Mysql）而非stdout输出的应用程序，客户端的终端显示的仅是作为YARN的job的简单运行状况。</p>
</blockquote>
</blockquote>

	

	

</article>




	<article>
	
		<h1><a href="/2022/05/15/hello-world/">Hello World</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-05-15</span><br />
		
		
	</div>

	

	
		<p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

	

	

</article>




	<article>
	
		<h1><a href="/2022/03/10/github基础/">Github基础</a></h1>
	
	<div class="article__infos">
		<span class="article__date">2022-03-10</span><br />
		
		
			<span class="article__tags">
			  	<a class="article__tag-none-link" href="/tags/%E6%9A%82%E6%97%B6%E5%81%9C%E6%9B%B4/" rel="tag">暂时停更</a>
			</span>
		
	</div>

	

	
		<h1 id="Git-基础操作-（4-1）"><a href="#Git-基础操作-（4-1）" class="headerlink" title="Git 基础操作 （4-1）"></a><strong>Git 基础操作 （4-1）</strong></h1><ul>
<li><h3 id="git-init-——–初始化仓库"><a href="#git-init-——–初始化仓库" class="headerlink" title="git init ——–初始化仓库"></a>git init ——–初始化仓库</h3></li>
</ul>
<p>在使用Git进行版本管理，必须先出实话仓库。出实话之前先创建一个目录（git-tutorial）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir git-tutorial</span><br><span class="line">$ cd git-tutorial</span><br><span class="line">$ git init</span><br><span class="line">hint: Using &#x27;master&#x27; as the name for the initial branch. This default branch name</span><br><span class="line">hint: is subject to change. To configure the initial branch name to use in all</span><br><span class="line">hint: of your new repositories, which will suppress this warning, call:</span><br><span class="line">hint: </span><br><span class="line">hint: 	git config --global init.defaultBranch &lt;name&gt;</span><br><span class="line">hint: </span><br><span class="line">hint: Names commonly chosen instead of &#x27;master&#x27; are &#x27;main&#x27;, &#x27;trunk&#x27; and</span><br><span class="line">hint: &#x27;development&#x27;. The just-created branch can be renamed via this command:</span><br><span class="line">hint: </span><br><span class="line">hint: 	git branch -m &lt;name&gt; </span><br></pre></td></tr></table></figure>

<p>代码执行成后，目录会生成.git目录（储存管理当前目录内容所需的数据仓库）</p>
<ul>
<li><h3 id="git-status-——-查看仓库状态"><a href="#git-status-——-查看仓库状态" class="headerlink" title="git status ——-查看仓库状态"></a>git status ——-查看仓库状态</h3></li>
</ul>
<h5 id="git-status命令用于显示Git仓库的状态。"><a href="#git-status命令用于显示Git仓库的状态。" class="headerlink" title="git status命令用于显示Git仓库的状态。"></a><strong>git status命令用于显示Git仓库的状态。</strong></h5><p>因工作树和仓库被操作的过程中，状态会不断发生变化。所以需要git status 命令查看当前状态。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git ststua</span><br><span class="line">On branch master        #当前处于master分支</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">nothing to commit (create/copy files and use &quot;git add&quot; to track)</span><br></pre></td></tr></table></figure>

<p>所谓提交（commit）：是指“ 记录工作数中所有文件的当前状态 ”</p>
<p>因为我们第一次操作没有可提交内容（没有记录任何文件的任何状态），所以我们建立README.md文件作为对象</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ touch README.md </span><br><span class="line">$ git status </span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed)</span><br><span class="line">	README.md</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use &quot;git add&quot; to track)</span><br></pre></td></tr></table></figure>

<p>可以看到在Untracked files中显示了README.md文件。以此类推只要对Git进行操作 <strong>git status</strong>显示的结果就会发变化。</p>
<ul>
<li><h3 id="git-add-———先暂时区中添加文件"><a href="#git-add-———先暂时区中添加文件" class="headerlink" title="git add ———先暂时区中添加文件"></a>git add ———先暂时区中添加文件</h3></li>
</ul>
<p> 如果仅仅用Git仓库的工作树创建了文件，那么该文件并不会被记入git仓库的版本管理对象中。所以当使用<strong>git status</strong>时会显示在Untracked files中。</p>
<p>若想将其成为Git仓库的管理对象，解决办法是使用 <strong>git add</strong>命令将其加入暂存区（stage or index） ==暂存区是提交之前的一个临时区域==</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ git add README.md </span><br><span class="line">$ git status</span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage)</span><br><span class="line">	new file:   README.md</span><br></pre></td></tr></table></figure>

<p>将README.md加入暂存区后，<strong>git status</strong>可看出README.md文件在changes to be committed 中。</p>
<ul>
<li><h3 id="git-commit-———保存仓库的历史记录"><a href="#git-commit-———保存仓库的历史记录" class="headerlink" title="git commit ———保存仓库的历史记录"></a>git commit ———保存仓库的历史记录</h3><p><strong>git commit</strong>命令可将当前出存在暂存区的文件保存到仓库的历史记录中。通过这些，我们可以在工作树中福源文件。</p>
<ul>
<li><p>记述一行提交信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;First commit&quot;</span><br><span class="line">[master (root-commit) 335488e] First commit</span><br><span class="line"> 1 file changed, 0 insertions(+), 0 deletions(-)</span><br><span class="line"> create mode 100644 README.md</span><br></pre></td></tr></table></figure>

<p>-m后的“First commit” 是对提交的概述.</p>
</li>
<li><p>记述详细提交信息</p>
<p>若想记录更详细，则不加 -m,执行后直接编辑即可</p>
<p>● 第一行：用一行文字简述提交的更改内容</p>
<p>● 第二行：空行</p>
<p>● 第三行以后：记述更改的原因和详细内容</p>
</li>
<li><p>中止提交</p>
<p>若在编译器启动后想终止，则将提交信息留空并关闭编译器即可</p>
</li>
</ul>
</li>
<li><h3 id="git-log-——–查看提交日志"><a href="#git-log-——–查看提交日志" class="headerlink" title="git log ——–查看提交日志"></a>git log ——–查看提交日志</h3><p><strong>git log</strong>可以查看以往仓库中提交的日志。如：什么人在什么时候进行了提交或合并，以及操作前后有怎样的差别。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git log </span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 09:47:09 2022 -0400</span><br><span class="line"></span><br><span class="line">    First commit</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong>git log –pretty=short</strong>不显示Date，方便把握多个提交。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git log </span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line"></span><br><span class="line">    First commit</span><br></pre></td></tr></table></figure></li>
<li><p>特定查找摸个文件</p>
<p><code>$ git log READMR.md</code></p>
</li>
<li><p>显示文件的改动</p>
<p>若想看提交所带来的改动怎加上 -p </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ git log -p</span><br><span class="line">$ git log -p README.md</span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 09:47:09 2022 -0400</span><br><span class="line"></span><br><span class="line">    First commit</span><br><span class="line"></span><br><span class="line">diff --git a/README.md b/README.md</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000..e69de29</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<ul>
<li><h3 id="git-diff-——–查看更改前后的差别"><a href="#git-diff-——–查看更改前后的差别" class="headerlink" title="git diff ——–查看更改前后的差别"></a>git diff ——–查看更改前后的差别</h3><p>先vim README.md</p>
<p><code># Git教程</code></p>
<ul>
<li><p>查看工作树和暂存区的差别</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git diff</span><br><span class="line">diff --git a/README.md b/README.md</span><br><span class="line">index e69de29..981aef2 100644</span><br><span class="line">--- a/README.md</span><br><span class="line">+++ b/README.md</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+#Git教程</span><br></pre></td></tr></table></figure>

<p>在最后一行出现的+（-）表示先添加的行（被删除）</p>
<p>用<strong>git add</strong>将md文件加入暂存区</p>
<p><code>$ git add READMD.md</code></p>
<ul>
<li><h4 id="查看工作树和最新提交的差别"><a href="#查看工作树和最新提交的差别" class="headerlink" title="查看工作树和最新提交的差别"></a>查看工作树和最新提交的差别</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ git diff HEAD</span><br><span class="line">diff --git a/README.md b/README.md</span><br><span class="line">index e69de29..981aef2 100644</span><br><span class="line">--- a/README.md</span><br><span class="line">+++ b/README.md</span><br><span class="line">@@ -0,0 +1 @@</span><br><span class="line">+#Git教程</span><br></pre></td></tr></table></figure>

<p><strong>养成好习惯：执行git commit 前先执行 git diff HEAD</strong></p>
<p>对比后进行提交 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;Add index&quot;</span><br><span class="line">[master fe4c8d9] Add index</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>

<p>保险起见再查看下一觉日志</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ git log</span><br><span class="line">commit fe4c8d999e632799390fc35ca8d9529e90940014 (HEAD -&gt; master)</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 10:02:02 2022 -0400</span><br><span class="line"></span><br><span class="line">    Add index</span><br><span class="line"></span><br><span class="line">commit 335488e5478c13f9cf0259a9a66bafae8de13b9d</span><br><span class="line">Author: notrip &lt;“1837357465@qq.com”&gt;</span><br><span class="line">Date:   Tue May 17 09:47:09 2022 -0400</span><br><span class="line"></span><br><span class="line">    First commit</span><br></pre></td></tr></table></figure>

<p>看到两个commit表示成功    </p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 id="分支的操作（4-2）"><a href="#分支的操作（4-2）" class="headerlink" title="分支的操作（4-2）"></a>分支的操作（4-2）</h1><ul>
<li><h3 id="git-branch-——–显示分支—-览表"><a href="#git-branch-——–显示分支—-览表" class="headerlink" title="git branch ——–显示分支—-览表"></a>git branch ——–显示分支—-览表</h3><p><strong>git branch</strong>显示当前所在分支</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git branch</span><br><span class="line">* master</span><br></pre></td></tr></table></figure></li>
<li><h3 id="git-checkout-b-———-创建、切换分支"><a href="#git-checkout-b-———-创建、切换分支" class="headerlink" title="git checkout -b ———-创建、切换分支"></a>git checkout -b ———-创建、切换分支</h3><ul>
<li><p>切换到feature -A分支并进行提交(两种方法)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git branch feature-A</span><br><span class="line">$ git checkout -b feature-A</span><br><span class="line">Switched to a new branch &#x27;feature-A&#x27;</span><br><span class="line">$ git branch</span><br><span class="line">* feature-A</span><br><span class="line">  master</span><br></pre></td></tr></table></figure>

<p><strong>不断对一个分支进行提交的操作被称为“培育分支”</strong></p>
<p>œ添加到feature-A分支中</p>
<blockquote>
<p>$ vim README.md</p>
<blockquote>
<p>#Git教程</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git commit -m &quot;Add feature-A&quot;</span><br><span class="line">[feature-A e5033d6] Add feature-A</span><br><span class="line"> 1 file changed, 2 insertions(+)</span><br></pre></td></tr></table></figure></blockquote>
</li>
<li><p>切换到master分支</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">Switched to branch &#x27;master&#x27;</span><br></pre></td></tr></table></figure>

<p>切换到master分支中看到README.md中的内容不变。因为feature-A不会影响master。</p>
</li>
<li><p>切换回上一个分支</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git checkout -</span><br><span class="line">Switched to branch &#x27;feature-A&#x27;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>最近只在学习github，一直没有整理，没有一直更新，不久就会更新 ！😭</p>

	

	

</article>






	<span class="different-posts">📕 end of posts 📕</span>


	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2022 notrip | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
