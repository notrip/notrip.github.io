<!DOCTYPE html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta name="theme-color" content="#33474d">
	<title>kafka API使用方法 | Hexo</title>
	<link rel="stylesheet" href="/css/style.css" />
	
      <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
    
<meta name="generator" content="Hexo 5.4.2"></head>

<body>

	<header class="header">
		<nav class="header__nav">
			
				<a href="/archives" class="header__link">Archive</a>
			
				<a href="/tags" class="header__link">Tags</a>
			
				<a href="/atom.xml" class="header__link">RSS</a>
			
		</nav>
		<h1 class="header__title"><a href="/">Hexo</a></h1>
		<h2 class="header__subtitle"></h2>
	</header>

	<main>
		<article>
	
		<h1>kafka API使用方法</h1>
	
	<div class="article__infos">
		<span class="article__date">2022-06-11</span><br />
		
		
	</div>

	

	
		<h5 id="博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1"><a href="#博主的本地环境为：Macbok-pro-macOS-Monterey-12-3-1" class="headerlink" title="博主的本地环境为：Macbok pro , macOS Monterey 12.3.1"></a><em>博主的本地环境为：Macbok pro , macOS Monterey 12.3.1</em></h5><hr>
<h4 id="1-首先引入maven文件"><a href="#1-首先引入maven文件" class="headerlink" title="1.首先引入maven文件"></a>1.首先引入maven文件</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;<span class="number">2.0</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">    &lt;dependency&gt;</span><br><span class="line">        &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">        &lt;artifactId&gt;kafka_2<span class="number">.11</span>&lt;/artifactId&gt;</span><br><span class="line">        &lt;version&gt;<span class="number">2.2</span><span class="number">.0</span>&lt;/version&gt;</span><br><span class="line">    &lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<p>⚠️：<strong>这里注意version和服务器的kafka版本要一致</strong></p>
<h4 id="2-生产者API"><a href="#2-生产者API" class="headerlink" title="2.生产者API"></a>2.生产者API</h4><h5 id="2-1"><a href="#2-1" class="headerlink" title="2.1"></a>2.1</h5><p><strong>创建生产者大概分为以下步骤</strong></p>
<blockquote>
<p>1.配置生产者客户端参数及创建相应的生产者实例<br>2.构建待发送的消息<br>3.发送消息<br>4.关闭生产者实例</p>
</blockquote>
<ul>
<li><p>代码示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">//设置 kafka 集群的地址</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//ack 模式,取值有 0,1,-1(all) , all 是最慢但最安全的，</span></span><br><span class="line">props.put(“acks”, “all”); </span><br><span class="line"></span><br><span class="line"><span class="comment">//失败重试次数-&gt;失败会自动重试（可恢复/不可恢复）--&gt;(有可能会造成数据的乱序)</span></span><br><span class="line">props.put(“retries”, <span class="number">3</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送的批次大小提高效率/吞吐量太大会数据延迟</span></span><br><span class="line">props.put(“batch.size”, <span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//消息在缓冲区保留的时间,超过设置的值就会被提交到服务端</span></span><br><span class="line">props.put(<span class="string">&quot;linger.ms&quot;</span>, <span class="number">10000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//数据发送请求的最大缓存数</span></span><br><span class="line">props.put(<span class="string">&quot;max.request.size&quot;</span>,<span class="number">10</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">//整个 Producer 用到总内存的大小,如果缓冲区满了会提交数据到服务端 buffer.memory 要大于 batch.size,否则会报申请内存不足的错误降低阻塞的可能性</span></span><br><span class="line">props.put(“buffer.memory”, <span class="number">10240</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//key-value序列化器</span></span><br><span class="line">props.put(<span class="string">&quot;key.serializer&quot;</span>, <span class="string">&quot;org.apache.kafka.common.serialization.StringSerializer&quot;</span>); </span><br></pre></td></tr></table></figure></li>
<li><p>消息对象 ProducerRecord,它并不是单纯意义上的消息,它包含了多个属性,原本需要发送的与业务关的消息体只是其中的一个 value 属性.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord 类的定义如下:</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Integer partition;</span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line">	<span class="keyword">private</span> <span class="keyword">final</span> Long timestamp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="2-2参数配置"><a href="#2-2参数配置" class="headerlink" title="2.2参数配置"></a>2.2参数配置</h5><ul>
<li>在创建真正的生产者实例前需要配置相应的参数,比如需要连接的 Kafka 集群地址。在 Kafka 生产者客户端 KatkaProducer 中有 3 个参数是必填的。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* bootstrap.servers </span><br><span class="line">* key.serializer </span><br><span class="line">* value.serializer</span><br></pre></td></tr></table></figure>

<ul>
<li>例如为了防止参数名字符串书写错误,可以进行以下设置:</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,ProducerInterceptorPrefix.class.getName());</span><br><span class="line">props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092&quot;</span>); </span><br><span class="line">props.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName()); props.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br></pre></td></tr></table></figure>

<h5 id="2-3生产者api参数发送方式"><a href="#2-3生产者api参数发送方式" class="headerlink" title="2.3生产者api参数发送方式"></a>2.3生产者api参数发送方式</h5><p>这个客户端经过了生产环境测试并且通常情况它比原来Scals客户端更加快速、功能更加齐全。你可以通过添加以下示例的Maven坐标到客户端依赖中来使用这个新的客户端</p>
<ul>
<li><p>发后即忘( fire-and-forget )</p>
<p>发后即忘,它只管往 Kafka 发送,并不关心消息是否正确到达。在大多数情况下,这种发送方式没有问题; 不过在某些时候(比如发生不可重试异常时)会造成消息的丢失。这种发送方式的性能最高,可靠性最差。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Future&lt;RecordMetadata&gt; send = producer.send(rcd);</span><br></pre></td></tr></table></figure></li>
<li><p>同步发送( sync )</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">	producer.send(rcd).get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">	e.printStackTrace();</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>在调用 <code>send</code> 方法后可以接着调用 <code>get()</code> 方法，<code>send</code> 方法的返回值是一个 Future\对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">        <span class="comment">/*同步发送消息*/</span></span><br><span class="line">        <span class="type">RecordMetadata</span> <span class="variable">metadata</span> <span class="operator">=</span> producer.send(record).get();</span><br><span class="line">        System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">    &#125; <span class="keyword">catch</span> (InterruptedException | ExecutionException e) &#123;</span><br><span class="line">        e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 <code>Hello-Kafka</code> 主题时候，使用 <code>--partitions</code> 指定其分区数为 1，即只有一个分区。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">topic=Hello-Kafka, partition=0, offset=40 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=41 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=42 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=43 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=44 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=45 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=46 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=47 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=48 </span><br><span class="line">topic=Hello-Kafka, partition=0, offset=49</span><br></pre></td></tr></table></figure></li>
<li><p>异步发送( async )</p>
<p>回调函数会在 producer 收到 ack 时调用,为异步调用,该方法有两个参数,分别是 RecordMetadata 和Exception,如果 Exception 为 null,说明消息发送成功,如果 Exception 不为 null,说明消息发送失败。</p>
<blockquote>
<p>⚠️：如若消息发送失败，会自动重新发送，无需手动使用回调函数</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">10</span>; i++) &#123;</span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> <span class="title class_">ProducerRecord</span>&lt;&gt;(topicName, <span class="string">&quot;k&quot;</span> + i, <span class="string">&quot;world&quot;</span> + i);</span><br><span class="line">    <span class="comment">/*异步发送消息，并监听回调*/</span></span><br><span class="line">    producer.send(record, <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata metadata, Exception exception)</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (exception != <span class="literal">null</span>) &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;进行异常处理&quot;</span>);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                System.out.printf(<span class="string">&quot;topic=%s, partition=%d, offset=%s \n&quot;</span>,</span><br><span class="line">                        metadata.topic(), metadata.partition(), metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<hr>
<h4 id="3-消费者API"><a href="#3-消费者API" class="headerlink" title="3.消费者API"></a>3.消费者API</h4><h5 id="3-1"><a href="#3-1" class="headerlink" title="3.1"></a>3.1</h5><p><strong>创建消费者大概分为以下步骤：</strong></p>
<blockquote>
<p>1.配置消费者客户端参数<br>2.创建相应的消费者实例;<br>3.订阅主题;<br>4.拉取消息并消费;<br>5.提交消费位移 offset;<br>6.关闭消费者实例。</p>
</blockquote>
<ul>
<li><p>代码示例：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 kakfa 服务的地址,不需要将所有 broker 指定上</span></span><br><span class="line">props.put(<span class="string">&quot;bootstrap.servers&quot;</span>, <span class="string">&quot;node1:9092&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定 consumer group </span></span><br><span class="line">props.put(<span class="string">&quot;group.id&quot;</span>, <span class="string">&quot;g1&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 是否自动提交 offset </span></span><br><span class="line">props.put(<span class="string">&quot;enable.auto.commit&quot;</span>, <span class="string">&quot;true&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 自动提交 offset 的时间间隔</span></span><br><span class="line">props.put(<span class="string">&quot;auto.commit.interval.ms&quot;</span>, <span class="string">&quot;1000&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// key 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;key.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// value 的反序列化类</span></span><br><span class="line">props.put(<span class="string">&quot;value.deserializer&quot;</span>,<span class="string">&quot;org.apache.kafka.common.serialization.StringDeserializer&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 如果没有消费偏移量记录,则自动重设为起始 offset:latest, earliest, none</span></span><br><span class="line"><span class="comment">//Earliest目前状态下最前面的一条消息（日志在一定保存时间后会自动清空）</span></span><br><span class="line"><span class="comment">//none（上次记录的偏移量，如果没有，会抛异常） </span></span><br><span class="line">props.put(<span class="string">&quot;auto.offset.reset&quot;</span>,<span class="string">&quot;earliest&quot;</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 定义 consumer KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); </span></span><br><span class="line"><span class="comment">// 消费者订阅的 topic, 可同时订阅多个</span></span><br><span class="line">consumer.subscribe(Arrays.asList(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;test&quot;</span>,<span class="string">&quot;test1&quot;</span>)); </span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="3-2"><a href="#3-2" class="headerlink" title="3.2"></a>3.2</h5><ul>
<li>Kafka消费者可选属性</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. fetch.min.byte</span><br><span class="line">消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。</span><br><span class="line"></span><br><span class="line">2. fetch.max.wait.ms</span><br><span class="line">broker 返回给消费者数据的等待时间，默认是 500ms。</span><br><span class="line"></span><br><span class="line">3. max.partition.fetch.bytes</span><br><span class="line">该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。</span><br><span class="line"></span><br><span class="line">4. session.timeout.ms</span><br><span class="line">消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。</span><br><span class="line"></span><br><span class="line">5. auto.offset.reset</span><br><span class="line">该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：</span><br><span class="line"></span><br><span class="line">latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）;</span><br><span class="line">earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。</span><br><span class="line">6. enable.auto.commit</span><br><span class="line">是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。</span><br><span class="line"></span><br><span class="line">7. client.id</span><br><span class="line">客户端 id，服务器用来识别消息的来源。</span><br><span class="line"></span><br><span class="line">8. max.poll.records</span><br><span class="line">单次调用 poll() 方法能够返回的记录数量。</span><br><span class="line"></span><br><span class="line">9. receive.buffer.bytes &amp; send.buffer.byte</span><br><span class="line">这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。</span><br></pre></td></tr></table></figure>

<ul>
<li>必要参数配置</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,brokerList);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,groupid);</span><br><span class="line"></span><br><span class="line">props.put(ConsumerConfig.CLIENT_ID_CONFIG,clientid);</span><br></pre></td></tr></table></figure>

<h5 id="3-3-subscribe-订阅主题"><a href="#3-3-subscribe-订阅主题" class="headerlink" title="3.3  subscribe 订阅主题"></a>3.3  subscribe 订阅主题</h5><ul>
<li><p>subscribe 的重载方式:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics,ConsumerRebalanceListener listener)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span> </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">subscribe</span><span class="params">(Pattern pattern)</span></span><br></pre></td></tr></table></figure></li>
<li><p>指定集合方式订阅主题</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic1)); </span><br><span class="line">consumer <span class="title function_">subscribe</span><span class="params">(Arrays.asList(topic2)</span>);</span><br></pre></td></tr></table></figure></li>
<li><p>正则方式订阅主题</p>
<p>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Pattern.compile (<span class="string">&quot;topic.*&quot;</span> )); </span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="3-4-assign-订阅主题"><a href="#3-4-assign-订阅主题" class="headerlink" title="3.4 assign 订阅主题"></a>3.4 assign 订阅主题</h5><p>消费者不仅可以通过 KafkaConsumer.subscribe() 方法订阅主题,还可直接订阅某些主题的指定分区;</p>
<p>在 KafkaConsumer 中提供了 assign() 方法来实现这些功能,此方法的具体定义如下:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span>;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这个方法只接受参数 partitions,用来指定需要订阅的分区集合。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> <span class="title class_">TopicPartition</span> (<span class="string">&quot;tpc_1&quot;</span> , <span class="number">0</span>),<span class="keyword">new</span> <span class="title class_">TopicPartition</span>(“tpc_2”,<span class="number">1</span>))) ;</span><br></pre></td></tr></table></figure>

<h5 id="3-5-subscribe-与-assign-的区别"><a href="#3-5-subscribe-与-assign-的区别" class="headerlink" title="3.5 subscribe 与 assign 的区别"></a>3.5 subscribe 与 assign 的区别</h5><ul>
<li><p>通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ;</p>
<blockquote>
<p>在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。</p>
</blockquote>
</li>
<li><p>assign() 方法订阅分区时,是不具备消费者自动均衡的功能的;</p>
<blockquote>
<p>其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有 ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。</p>
</blockquote>
</li>
</ul>
<h5 id="3-6-取消订阅"><a href="#3-6-取消订阅" class="headerlink" title="3.6 取消订阅"></a>3.6 取消订阅</h5><p>可以使用 KafkaConsumer 中的 unsubscribe()方法采取消主题的订阅,这个方法既可以取消通过subscribe( Collection)方式实现的订阅; 也可以取消通过 subscribe(Pattem)方式实现的订阅,还可以取消通过 assign( Collection)方式实现的订阅。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br></pre></td></tr></table></figure>

<p>如果将 subscribe(Collection )或 assign(Collection)集合参数设置为空集合,作用与 unsubscribe()方法相同,如下示例中三行代码的效果相同:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe(); </span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;String&gt;()) ; </span><br><span class="line">consumer.assign(<span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>

<h5 id="3-7-消息的消费模式"><a href="#3-7-消息的消费模式" class="headerlink" title="3.7 消息的消费模式"></a>3.7 消息的消费模式</h5><p>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息。</p>
<p>对于 poll () 方法而言,如果某些分区中没有可供消费的消息,那么此分区对应的消息拉取的结果就为空如果订阅的所有分区中都没有可供消费的消息,那么 poll()方法返回为空的消息集; poll () 方法具体定义如下:<br>public ConsumerRecords&lt;K, V&gt; poll(final Duration timeout)<br>超时时间参数 timeout , 用来控制 poll() 方法的阻塞时间, 在消费者的缓冲区里没有可用数据时会发生阻塞。如果消费者程序只用来单纯拉取并消费数据,则为了提高吞吐率,可以把 timeout 设置为Long.MAX_VALUE;</p>
<p>​            <em><strong>消费者消费到的每条消息的类型为 ConsumerRecord</strong></em></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ConsumerRecord</span>&lt;K, V&gt; &#123; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">long</span> <span class="variable">NO_TIMESTAMP</span> <span class="operator">=</span> RecordBatch.NO_TIMESTAMP; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_SIZE</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="type">int</span> <span class="variable">NULL_CHECKSUM</span> <span class="operator">=</span> -<span class="number">1</span>; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> String topic; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> partition; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> offset;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">long</span> timestamp; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedKeySize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> <span class="type">int</span> serializedValueSize; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Headers headers; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> K key; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> V value; </span><br><span class="line"><span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>topic partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</p>
<p>offsset 表示消息在所属分区的偏移量。</p>
<p>timestamp 表示时间戳,与此对应的 timestampType 表示时间戳的类型。</p>
<p>timestampType 有两种类型 CreateTime 和 LogAppendTime , 分别代表消息创建的时间戳和消息追加到日志的时间戳。</p>
<p>headers 表示消息的头部内容。</p>
<p>key value 分别表示消息的键和消息的值,一般业务应用要读取的就是 value ; </p>
<p>serializedKeySize、serializedValueSize 分别表示 key、value 经过序列化之后的大小,如果 key 为空, 则 serializedKeySize 值为 -1,同样,如果 value 为空,则 serializedValueSize 的值也会为 -1; </p>
<p>checksum 是 CRC32 的校验值。</p>
</blockquote>
<h5 id="3-8指定位移消费"><a href="#3-8指定位移消费" class="headerlink" title="3.8指定位移消费"></a>3.8指定位移消费</h5><p>有些时候,我们需要一种更细粒度的掌控,可以让我们从特定的位移处开始拉取消息,而KafkaConsumer 中的 seek() 方法正好提供了这个功能,让我们可以追前消费或回溯消费。</p>
<p>​                <em><strong>seek()方法:</strong></em>        </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">seek</span><span class="params">(TopicPartiton partition,<span class="type">long</span> offset)</span>;</span><br></pre></td></tr></table></figure>

<h5 id="3-9-再均衡监听器"><a href="#3-9-再均衡监听器" class="headerlink" title="3.9  再均衡监听器"></a>3.9  再均衡监听器</h5><p>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡; 如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p>
<h5 id="3-10自动位移提交"><a href="#3-10自动位移提交" class="headerlink" title="3.10自动位移提交"></a>3.10自动位移提交</h5><p>Kafka 中默认的消费位移的提交方式是自动提交,这个由消费者客户端参数 enable.auto.commit 配置, 默认值为 true 。当然这个默认的自动提交不是每消费一条消息就提交一次,而是定期提交,这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置, 默认值为 5 秒, 此参数生效的前提是 enable.</p>
<p>auto.commit 参数为 true。</p>
<p>在默认的方式下,消费者每隔 5 秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的,在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交,如果可以,那么就会提交上一次轮询的位移。</p>
<p>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。</p>
<ul>
<li><p>重复消费</p>
<p>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。</p>
</li>
<li><p>丢失消息</p>
<ul>
<li>按照一般思维逻辑而言,自动提交是延时提交,重复消费可以理解,那么消息丢失又是在什么情形下会发生的呢?我们来看下图中的情形: 拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理。设目前进行到了第 y+l 次拉取,以及第 m 次位移提交的时候,也就是x+6 之前的位移己经确认提交了, 处理线程却还正在处理 x+3 的消息; 此时如果处理线程发生了异常, 待其恢复之后会从第 m 次位移提交处,也就是 x+6 的位置开始拉取消息,那么 x+3 至 x+6 之间的消息就没有得到相应的处理,这样便发生消息丢失的现象。</li>
</ul>
</li>
</ul>
<h5 id="3-11手动位移提交-调用-kafka-api"><a href="#3-11手动位移提交-调用-kafka-api" class="headerlink" title="3.11手动位移提交(调用 kafka api)"></a>3.11手动位移提交(调用 kafka api)</h5><p>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象, 但是在编程的世界里异常无可避免; 同时, 自动位移提交也无法做到精确的位移管理。 在 Kafka 中还提供了手动位移提交的方式, 这样可以使得开发人员对消费位移的管理控制更加灵活。<br>很多时候并不是说拉取到消息就算消费完成,而是需要将消息写入数据库、写入本地缓存,或者是更加复杂的业务处理。在这些场景下,所有的业务处理完成才能认为消息被成功消费; 手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。 开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 fals 。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConf.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">false</span>); </span><br></pre></td></tr></table></figure>

<p>⚠️：<strong>手动提交可以细分为同步提交和异步提交,对应于 KafkaConsumer 中的 commitSync()和commitAsync()两种类型的方法。</strong></p>
<hr>
<h4 id="3-Topic管理-API"><a href="#3-Topic管理-API" class="headerlink" title="3.Topic管理 API"></a>3.Topic管理 API</h4><p>一般情况下,我们都习惯使用 kafka-topic.sh 本来管理主题,如果希望将管理类的功能集成到公司内部的系统中,打造集管理、监控、运维、告警为一体的生态平台,那么就需要以程序调用 API 方式去实现。这种调用 API 方式实现管理主要利用 KafkaAdminClient 工具类KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)</p>
<h5 id="3-1列出主题"><a href="#3-1列出主题" class="headerlink" title="3.1列出主题"></a>3.1列出主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">ListTopicsResult</span> <span class="variable">listTopicsResult</span> <span class="operator">=</span> adminClient.listTopics(); </span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; topics = listTopicsResult.names().get(); </span><br><span class="line"></span><br><span class="line">System.out.println(topics);</span><br></pre></td></tr></table></figure>

<h5 id="3-2查看主题信息"><a href="#3-2查看主题信息" class="headerlink" title="3.2查看主题信息"></a>3.2查看主题信息</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DescribeTopicsResult</span> <span class="variable">describeTopicsResult</span> <span class="operator">=</span> adminClient.describeTopics(Arrays.asList(<span class="string">&quot;tpc_4&quot;</span>, <span class="string">&quot;tpc_3&quot;</span>)); </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Map&lt;String, TopicDescription&gt; res = describeTopicsResult.all().get();</span><br><span class="line"></span><br><span class="line">Set&lt;String&gt; ksets = res.keySet(); </span><br><span class="line"><span class="keyword">for</span> (String k : ksets) &#123; </span><br><span class="line">	System.out.println(res.get(k)); </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h5 id="3-3-创建主题"><a href="#3-3-创建主题" class="headerlink" title="3.3 创建主题"></a>3.3 创建主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 参数配置</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">props</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>(); </span><br><span class="line">props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;node1:9092,node2:9092,node3:9092&quot;</span>); </span><br><span class="line">props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG,<span class="number">3000</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 创建 admin client 对象</span></span><br><span class="line"><span class="type">AdminClient</span> <span class="variable">adminClient</span> <span class="operator">=</span> KafkaAdminClient.create(props); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 由服务端 controller 自行分配分区及副本所在 broker </span></span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_3</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_3&quot;</span>, <span class="number">2</span>, (<span class="type">short</span>) <span class="number">1</span>); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 手动指定分区及副本的 broker 分配</span></span><br><span class="line">HashMap&lt;Integer, List&lt;Integer&gt;&gt; replicaAssignments = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;(); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 分区 0,分配到 broker0,broker1 replicaAssignments.put(0,Arrays.asList(0,1)); </span></span><br><span class="line"><span class="comment">// 分区 1,分配到 broker0,broker2 </span></span><br><span class="line">replicaAssignments.put(<span class="number">0</span>,Arrays.asList(<span class="number">0</span>,<span class="number">1</span>));</span><br><span class="line"><span class="type">NewTopic</span> <span class="variable">tpc_4</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">NewTopic</span>(<span class="string">&quot;tpc_4&quot;</span>, replicaAssignments); </span><br><span class="line"><span class="type">CreateTopicsResult</span> <span class="variable">result</span> <span class="operator">=</span> adminClient.createTopics(Arrays.asList(tpc_3,tpc_4)); </span><br><span class="line"></span><br><span class="line"><span class="comment">// 从 future 中等待服务端返回</span></span><br><span class="line"><span class="keyword">try</span> &#123; </span><br><span class="line">	result.all().get(); </span><br><span class="line">&#125; <span class="keyword">catch</span> (Exception e) &#123; </span><br><span class="line">e.printStackTrace(); </span><br><span class="line">&#125; </span><br><span class="line">adminClient.close();</span><br></pre></td></tr></table></figure>

<h5 id="3-4删除主题"><a href="#3-4删除主题" class="headerlink" title="3.4删除主题"></a>3.4删除主题</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">DeleteTopicsResult</span> <span class="variable">deleteTopicsResult</span> <span class="operator">=</span> adminClient.deleteTopics(Arrays.asList(<span class="string">&quot;tpc_1&quot;</span>, <span class="string">&quot;tpc_1&quot;</span>)); </span><br><span class="line"></span><br><span class="line">Map&lt;String, KafkaFuture&lt;Void&gt;&gt; values = deleteTopicsResult.values();</span><br><span class="line"></span><br><span class="line">System.out.println(values);</span><br></pre></td></tr></table></figure>




	

	
		<span class="different-posts"><a href="/2022/06/11/kafka-API%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/" onclick="window.history.go(-1); return false;">⬅️ Go back </a></span>

	

</article>

	</main>

	<footer class="footer">
	<div class="footer-content">
		
	      <div class="footer__element">
	<p>Hi there, <br />welcome to my Blog glad you found it. Have a look around, will you?</p>
</div>

	    
	      <div class="footer__element">
	<h5>Check out</h5>
	<ul class="footer-links">
		<li class="footer-links__link"><a href="/archives">Archive</a></li>
		
		  <li class="footer-links__link"><a href="/atom.xml">RSS</a></li>
	    
		<li class="footer-links__link"><a href="/about">about page</a></li>
		<li class="footer-links__link"><a href="/tags">Tags</a></li>
		<li class="footer-links__link"><a href="/categories">Categories</a></li>
	</ul>
</div>

	    

		<div class="footer-credit">
			<span>© 2022 notrip | Powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> | Theme <a target="_blank" rel="noopener" href="https://github.com/HoverBaum/meilidu-hexo">MeiliDu</a></span>
		</div>

	</div>


</footer>



</body>

</html>
